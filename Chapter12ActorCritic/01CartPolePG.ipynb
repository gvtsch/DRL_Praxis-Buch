{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole mit PolicyGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTAN (Practical Deep Reinforcement Learning) ist eine Python-Bibliothek, die beim Training von künstlichen neuronalen Netzen für Verstärkungslernen hilft. Es stellt verschiedene nützliche Funktionen und Tools zur Verfügung, um das Training und die Auswertung von Reinforcement-Learning-Modellen zu erleichtern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der SummaryWriter ermöglicht es, Summaries für TensorFlow-Graphen und -Variablen zu erstellen und in ein  bestimmtes Verzeichnis zu schreiben. Diese Summaries können dann von einem Tool wie TensorBoard geladen und visualisiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8\n",
    "REWARD_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "        # super ist der Aufruf der Init der Superklasse, hier nn.Module\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128), # Linear ist wie Dense aus Tensorflow\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--baseline'], dest='baseline', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='Enable mean baseline', metavar=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--baseline\", default=False, action=\"store_true\", help=\"Enable mean baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Methode add_argument() werden dem Parser Argumente hinzugefügt. Im gegebenen Beispiel wird das Argument `\"--baseline\"` definiert. Durch das Setzen von `\"default=False\"` wird festgelegt, dass dieses Argument standardmäßig den Wert False hat, falls es nicht in der Kommandozeile angegeben wird. Mit `\"action='store_true'\"` wird festgelegt, dass das Vorhandensein des Argumentes `'--baseline'` dazu führt, dass der Wert auf `True` gesetzt wird.\n",
    "    \n",
    "Die zusätzliche Angabe von `\"help\"` gibt eine Beschreibung des Arguments, die in der Hilfe ausgegeben wird, wenn der Benutzer die Option --help verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir das Beispiel ausführen und als Kommandozeilenargument `\"--baseline\"` angeben, wird `args.baseline` den Wert `True` haben. Andernfalls wird `args.baseline` den Standardwert False haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment=\"-cartpole-pg\" + \"-baseline=%s\" % args.baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PGN(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor, apply_softmax=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolicyAgent\n",
    "Der PolicyAgent ist ein Schlüsselelement im Verstärkungslernen, da er die Strategie des Agenten definiert, dh wie der Agent Entscheidungen trifft, welche Aktionen er ausführt.\n",
    "\n",
    "Der PolicyAgent wird verwendet, um die Richtlinie oder Strategie des neuronalen Netzes zu steuern. Das neuronale Netz (net) wird als Argument übergeben, um die Vorhersagen zu erhalten. Der preprocessor-Parameter bestimmt, wie die Eingabedaten vor der Verwendung durch das neuronale Netzwerk vorverarbeitet werden. Mit `ptan.agent.float32_preprocessor` wird eine Funktion angegeben, die die Eingabedaten in den Datentyp `float32` umwandelt. Die Option `apply_softmax=True` bestimmt, ob die Softmax-Funktion auf die Ausgabe des neuronalen Netzes angewendet werden soll, um die Wahrscheinlichkeiten für jede Aktion zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Buffer\n",
    "Die ExperienceSourceFirstLast-Klasse speichert eine festgelegte Anzahl von Schritten (`steps_count`) der Erfahrung des Agents (Zustände, Aktionen, Belohnungen, nächste Zustände) in einem internen Puffer. Bei jedem Schritt des Agenten wird eine neue Erfahrung in den Puffer geschrieben, und wenn die Anzahl der gespeicherten Schritte die festgelegte Puffergröße überschreitet, werden die ältesten Erfahrungen entfernt.\n",
    "\n",
    "Während der Trainingsschleife werden die Erfahrungen aus dem Puffer genommen und verwendet, um das Netzwerk zu trainieren. Der Puffer hilft dabei, Erfahrungen zu speichern und in einer zufälligen Reihenfolge auf die Erfahrungen zuzugreifen, um das Training zu diversifizieren und das Wiederholen von Erfahrungen zu vermeiden.\n",
    "\n",
    "Der Puffer ermöglicht es dem Agenten, frühere Erfahrungen wiederzuverwenden und von ihnen zu lernen, was zur Verbesserung der Entscheidungsfindung führen kann. Durch das Speichern von Erfahrungen über mehrere Schritte können auch zeitliche Zusammenhänge und kumulative Belohnungen im Training berücksichtigt werden.\n",
    "\n",
    "Die Befüllung des `exp_source` Objekts findet statt, wenn es initialisiert wird. In der Zeile `exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)` wird eine Episode von Erfahrungen generiert, indem der Agent agent mit der Umgebung env interagiert. Das Objekt `exp_source` speichert diese generierten Erfahrungen für den weiteren Gebrauch in der Trainingsschleife.\n",
    "\n",
    "Die `exp_source`-Instanz, genauer gesagt die `ExperienceSourceFirstLast`-Klasse, speichert die einzelnen Erfahrungen des Agenten (Zustände, Aktionen, Belohnungen usw.) in einem Puffer. Dabei werden die zuvor verwendeten Erfahrungen mit jedem neuen Schritt aktualisiert, sodass ältere Erfahrungen schrittweise verfallen und aus dem Puffer entfernt werden.\n",
    "\n",
    "Die `pop_total_rewards()`-Methode wird verwendet, um die Gesamtbelohnungen der abgeschlossenen Episoden aus `exp_source` zu extrahieren und zurückzugeben. Dies ermöglicht die Berechnung von Durchschnittsbelohnungen oder anderen Messwerten, um den Fortschritt des Trainings zu verfolgen. Die Methode hat jedoch keinen direkten Einfluss auf den Inhalt des Puffers in exp_source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer\n",
    "\n",
    "Der Adam-Optimizer ist ein Optimierungsalgorithmus, der häufig in neuronalen Netzwerken verwendet wird. Er kombiniert Ideen aus dem Stochastic Gradient Descent (SGD) und dem RMSprop-Algorithmus.\n",
    "\n",
    "Der Adam-Algorithmus berechnet adaptive Lernraten für jeden Parameter des neuronalen Netzwerks. Der Name `Adam` steht für \"Adaptive Moment Estimation\". Hier ist der grundlegende Ablauf des Adam-Optimierers:\n",
    "1. Initialisierung der Parameter: Der Optimierer speichert zwei Zustandsvariablen `m` und `v`, die den ersten und den zweiten Moment des Gradienten schätzen.\n",
    "2. Berechnung des Gradienten: Der Gradient der Verlustfunktion in Bezug auf die Parameter des Netzwerks wird berechnet.\n",
    "3. Aktualisierung der Zustände `m` und `v`: Die Zustandsvariablen `m` und `v` werden aktualisiert, um die Momente des Gradienten zu schätzen. Das geschieht durch die Verwendung von Exponential Moving Averages der Gradienten und ihren Quadraten.\n",
    "4. Berechnung des Bias-korrigierten ersten und zweiten Moments: Da die Schätzer `m` und `v` zu Beginn des Algorithmus stark abgeschwächt sind, führt der Adam-Algorithmus eine Korrektur durch, um den Bias in den Anfangswerten zu beheben.\n",
    "5. Aktualisierung der Parameter: Die Parameter des Netzwerks werden mit Hilfe der berechneten adaptive Lernraten aktualisiert.\n",
    "\n",
    "Der Adam-Optimizer hat sich als effizienter und robuster in der Praxis erwiesen als der SGD und der RMSprop-Algorithmus, da er adaptive Lernraten verwendet und die Momente des Gradienten schätzt. Dadurch kann er schneller konvergieren und bessere Generalisierungseigenschaften aufweisen.\n",
    "\n",
    "#### Momente\n",
    "\n",
    "In Bezug auf den Adam-Optimizer bezieht sich der Begriff \"Moment\" auf statistische Maße, die verwendet werden, um den Verlauf des Gradienten über mehrere Iterationen hinweg zu schätzen. Der Adam-Algorithmus verwendet zwei Momente - das erste und das zweite Moment des Gradienten.\n",
    "\n",
    "Das erste Moment, oft als \"mittlerer Gradient\" bezeichnet, ist eine Schätzung des Durchschnitts des Gradients. Es wird durch die exponentielle Glättung der vergangenen Gradientschätzungen berechnet.\n",
    "\n",
    "Das zweite Moment, oft als \"variable Veränderung\" bezeichnet, ist eine Schätzung des Durchschnitts der quadrierten Gradientschwankungen. Es wird auch durch die exponentielle Glättung der vergangenen Gradientschätzungen berechnet.\n",
    "\n",
    "Diese beiden Momente werden verwendet, um die adaptiven Lernraten für die Aktualisierung der Parameter des neuronalen Netzwerks zu berechnen. Durch die Verwendung von Momenten kann der Adam-Optimizer Informationen über die Richtung und Stärke des Gradienten erhalten und so eine effektive Anpassung der Lernraten ermöglichen. Dies trägt zu einer effizienteren Optimierung des neuronalen Netzwerks bei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "step_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "reward_sum = 0.\n",
    "batch_states, batch_actions, batch_scales = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: reward:  16.00, mean_100:  16.00, episodes: 1\n",
      "31: reward:  15.00, mean_100:  15.50, episodes: 2\n",
      "54: reward:  23.00, mean_100:  18.00, episodes: 3\n",
      "70: reward:  16.00, mean_100:  17.50, episodes: 4\n",
      "84: reward:  14.00, mean_100:  16.80, episodes: 5\n",
      "98: reward:  14.00, mean_100:  16.33, episodes: 6\n",
      "113: reward:  15.00, mean_100:  16.14, episodes: 7\n",
      "133: reward:  20.00, mean_100:  16.62, episodes: 8\n",
      "148: reward:  15.00, mean_100:  16.44, episodes: 9\n",
      "171: reward:  23.00, mean_100:  17.10, episodes: 10\n",
      "212: reward:  41.00, mean_100:  19.27, episodes: 11\n",
      "234: reward:  22.00, mean_100:  19.50, episodes: 12\n",
      "244: reward:  10.00, mean_100:  18.77, episodes: 13\n",
      "258: reward:  14.00, mean_100:  18.43, episodes: 14\n",
      "275: reward:  17.00, mean_100:  18.33, episodes: 15\n",
      "289: reward:  14.00, mean_100:  18.06, episodes: 16\n",
      "390: reward: 101.00, mean_100:  22.94, episodes: 17\n",
      "412: reward:  22.00, mean_100:  22.89, episodes: 18\n",
      "426: reward:  14.00, mean_100:  22.42, episodes: 19\n",
      "448: reward:  22.00, mean_100:  22.40, episodes: 20\n",
      "467: reward:  19.00, mean_100:  22.24, episodes: 21\n",
      "492: reward:  25.00, mean_100:  22.36, episodes: 22\n",
      "505: reward:  13.00, mean_100:  21.96, episodes: 23\n",
      "534: reward:  29.00, mean_100:  22.25, episodes: 24\n",
      "546: reward:  12.00, mean_100:  21.84, episodes: 25\n",
      "560: reward:  14.00, mean_100:  21.54, episodes: 26\n",
      "576: reward:  16.00, mean_100:  21.33, episodes: 27\n",
      "590: reward:  14.00, mean_100:  21.07, episodes: 28\n",
      "602: reward:  12.00, mean_100:  20.76, episodes: 29\n",
      "632: reward:  30.00, mean_100:  21.07, episodes: 30\n",
      "653: reward:  21.00, mean_100:  21.06, episodes: 31\n",
      "684: reward:  31.00, mean_100:  21.38, episodes: 32\n",
      "700: reward:  16.00, mean_100:  21.21, episodes: 33\n",
      "714: reward:  14.00, mean_100:  21.00, episodes: 34\n",
      "740: reward:  26.00, mean_100:  21.14, episodes: 35\n",
      "756: reward:  16.00, mean_100:  21.00, episodes: 36\n",
      "773: reward:  17.00, mean_100:  20.89, episodes: 37\n",
      "787: reward:  14.00, mean_100:  20.71, episodes: 38\n",
      "819: reward:  32.00, mean_100:  21.00, episodes: 39\n",
      "834: reward:  15.00, mean_100:  20.85, episodes: 40\n",
      "851: reward:  17.00, mean_100:  20.76, episodes: 41\n",
      "883: reward:  32.00, mean_100:  21.02, episodes: 42\n",
      "894: reward:  11.00, mean_100:  20.79, episodes: 43\n",
      "905: reward:  11.00, mean_100:  20.57, episodes: 44\n",
      "923: reward:  18.00, mean_100:  20.51, episodes: 45\n",
      "937: reward:  14.00, mean_100:  20.37, episodes: 46\n",
      "951: reward:  14.00, mean_100:  20.23, episodes: 47\n",
      "968: reward:  17.00, mean_100:  20.17, episodes: 48\n",
      "980: reward:  12.00, mean_100:  20.00, episodes: 49\n",
      "1001: reward:  21.00, mean_100:  20.02, episodes: 50\n",
      "1009: reward:   8.00, mean_100:  19.78, episodes: 51\n",
      "1039: reward:  30.00, mean_100:  19.98, episodes: 52\n",
      "1063: reward:  24.00, mean_100:  20.06, episodes: 53\n",
      "1075: reward:  12.00, mean_100:  19.91, episodes: 54\n",
      "1084: reward:   9.00, mean_100:  19.71, episodes: 55\n",
      "1095: reward:  11.00, mean_100:  19.55, episodes: 56\n",
      "1112: reward:  17.00, mean_100:  19.51, episodes: 57\n",
      "1123: reward:  11.00, mean_100:  19.36, episodes: 58\n",
      "1138: reward:  15.00, mean_100:  19.29, episodes: 59\n",
      "1152: reward:  14.00, mean_100:  19.20, episodes: 60\n",
      "1187: reward:  35.00, mean_100:  19.46, episodes: 61\n",
      "1210: reward:  23.00, mean_100:  19.52, episodes: 62\n",
      "1221: reward:  11.00, mean_100:  19.38, episodes: 63\n",
      "1238: reward:  17.00, mean_100:  19.34, episodes: 64\n",
      "1262: reward:  24.00, mean_100:  19.42, episodes: 65\n",
      "1289: reward:  27.00, mean_100:  19.53, episodes: 66\n",
      "1305: reward:  16.00, mean_100:  19.48, episodes: 67\n",
      "1341: reward:  36.00, mean_100:  19.72, episodes: 68\n",
      "1352: reward:  11.00, mean_100:  19.59, episodes: 69\n",
      "1368: reward:  16.00, mean_100:  19.54, episodes: 70\n",
      "1395: reward:  27.00, mean_100:  19.65, episodes: 71\n",
      "1416: reward:  21.00, mean_100:  19.67, episodes: 72\n",
      "1426: reward:  10.00, mean_100:  19.53, episodes: 73\n",
      "1450: reward:  24.00, mean_100:  19.59, episodes: 74\n",
      "1462: reward:  12.00, mean_100:  19.49, episodes: 75\n",
      "1518: reward:  56.00, mean_100:  19.97, episodes: 76\n",
      "1530: reward:  12.00, mean_100:  19.87, episodes: 77\n",
      "1580: reward:  50.00, mean_100:  20.26, episodes: 78\n",
      "1591: reward:  11.00, mean_100:  20.14, episodes: 79\n",
      "1612: reward:  21.00, mean_100:  20.15, episodes: 80\n",
      "1635: reward:  23.00, mean_100:  20.19, episodes: 81\n",
      "1645: reward:  10.00, mean_100:  20.06, episodes: 82\n",
      "1666: reward:  21.00, mean_100:  20.07, episodes: 83\n",
      "1678: reward:  12.00, mean_100:  19.98, episodes: 84\n",
      "1705: reward:  27.00, mean_100:  20.06, episodes: 85\n",
      "1726: reward:  21.00, mean_100:  20.07, episodes: 86\n",
      "1738: reward:  12.00, mean_100:  19.98, episodes: 87\n",
      "1761: reward:  23.00, mean_100:  20.01, episodes: 88\n",
      "1783: reward:  22.00, mean_100:  20.03, episodes: 89\n",
      "1800: reward:  17.00, mean_100:  20.00, episodes: 90\n",
      "1813: reward:  13.00, mean_100:  19.92, episodes: 91\n",
      "1849: reward:  36.00, mean_100:  20.10, episodes: 92\n",
      "1866: reward:  17.00, mean_100:  20.06, episodes: 93\n",
      "1894: reward:  28.00, mean_100:  20.15, episodes: 94\n",
      "1919: reward:  25.00, mean_100:  20.20, episodes: 95\n",
      "1964: reward:  45.00, mean_100:  20.46, episodes: 96\n",
      "1988: reward:  24.00, mean_100:  20.49, episodes: 97\n",
      "2032: reward:  44.00, mean_100:  20.73, episodes: 98\n",
      "2072: reward:  40.00, mean_100:  20.93, episodes: 99\n",
      "2095: reward:  23.00, mean_100:  20.95, episodes: 100\n",
      "2126: reward:  31.00, mean_100:  21.10, episodes: 101\n",
      "2168: reward:  42.00, mean_100:  21.37, episodes: 102\n",
      "2197: reward:  29.00, mean_100:  21.43, episodes: 103\n",
      "2223: reward:  26.00, mean_100:  21.53, episodes: 104\n",
      "2263: reward:  40.00, mean_100:  21.79, episodes: 105\n",
      "2286: reward:  23.00, mean_100:  21.88, episodes: 106\n",
      "2299: reward:  13.00, mean_100:  21.86, episodes: 107\n",
      "2367: reward:  68.00, mean_100:  22.34, episodes: 108\n",
      "2422: reward:  55.00, mean_100:  22.74, episodes: 109\n",
      "2466: reward:  44.00, mean_100:  22.95, episodes: 110\n",
      "2512: reward:  46.00, mean_100:  23.00, episodes: 111\n",
      "2532: reward:  20.00, mean_100:  22.98, episodes: 112\n",
      "2566: reward:  34.00, mean_100:  23.22, episodes: 113\n",
      "2604: reward:  38.00, mean_100:  23.46, episodes: 114\n",
      "2621: reward:  17.00, mean_100:  23.46, episodes: 115\n",
      "2653: reward:  32.00, mean_100:  23.64, episodes: 116\n",
      "2671: reward:  18.00, mean_100:  22.81, episodes: 117\n",
      "2715: reward:  44.00, mean_100:  23.03, episodes: 118\n",
      "2748: reward:  33.00, mean_100:  23.22, episodes: 119\n",
      "2808: reward:  60.00, mean_100:  23.60, episodes: 120\n",
      "2844: reward:  36.00, mean_100:  23.77, episodes: 121\n",
      "2874: reward:  30.00, mean_100:  23.82, episodes: 122\n",
      "2954: reward:  80.00, mean_100:  24.49, episodes: 123\n",
      "3046: reward:  92.00, mean_100:  25.12, episodes: 124\n",
      "3071: reward:  25.00, mean_100:  25.25, episodes: 125\n",
      "3128: reward:  57.00, mean_100:  25.68, episodes: 126\n",
      "3185: reward:  57.00, mean_100:  26.09, episodes: 127\n",
      "3211: reward:  26.00, mean_100:  26.21, episodes: 128\n",
      "3234: reward:  23.00, mean_100:  26.32, episodes: 129\n",
      "3298: reward:  64.00, mean_100:  26.66, episodes: 130\n",
      "3395: reward:  97.00, mean_100:  27.42, episodes: 131\n",
      "3561: reward: 166.00, mean_100:  28.77, episodes: 132\n",
      "3589: reward:  28.00, mean_100:  28.89, episodes: 133\n",
      "3613: reward:  24.00, mean_100:  28.99, episodes: 134\n",
      "3648: reward:  35.00, mean_100:  29.08, episodes: 135\n",
      "3678: reward:  30.00, mean_100:  29.22, episodes: 136\n",
      "3720: reward:  42.00, mean_100:  29.47, episodes: 137\n",
      "3805: reward:  85.00, mean_100:  30.18, episodes: 138\n",
      "3876: reward:  71.00, mean_100:  30.57, episodes: 139\n",
      "3908: reward:  32.00, mean_100:  30.74, episodes: 140\n",
      "3982: reward:  74.00, mean_100:  31.31, episodes: 141\n",
      "4036: reward:  54.00, mean_100:  31.53, episodes: 142\n",
      "4117: reward:  81.00, mean_100:  32.23, episodes: 143\n",
      "4180: reward:  63.00, mean_100:  32.75, episodes: 144\n",
      "4241: reward:  61.00, mean_100:  33.18, episodes: 145\n",
      "4265: reward:  24.00, mean_100:  33.28, episodes: 146\n",
      "4348: reward:  83.00, mean_100:  33.97, episodes: 147\n",
      "4427: reward:  79.00, mean_100:  34.59, episodes: 148\n",
      "4463: reward:  36.00, mean_100:  34.83, episodes: 149\n",
      "4492: reward:  29.00, mean_100:  34.91, episodes: 150\n",
      "4527: reward:  35.00, mean_100:  35.18, episodes: 151\n",
      "4584: reward:  57.00, mean_100:  35.45, episodes: 152\n",
      "4597: reward:  13.00, mean_100:  35.34, episodes: 153\n",
      "4682: reward:  85.00, mean_100:  36.07, episodes: 154\n",
      "4743: reward:  61.00, mean_100:  36.59, episodes: 155\n",
      "4767: reward:  24.00, mean_100:  36.72, episodes: 156\n",
      "4806: reward:  39.00, mean_100:  36.94, episodes: 157\n",
      "4895: reward:  89.00, mean_100:  37.72, episodes: 158\n",
      "4916: reward:  21.00, mean_100:  37.78, episodes: 159\n",
      "4941: reward:  25.00, mean_100:  37.89, episodes: 160\n",
      "4998: reward:  57.00, mean_100:  38.11, episodes: 161\n",
      "5033: reward:  35.00, mean_100:  38.23, episodes: 162\n",
      "5076: reward:  43.00, mean_100:  38.55, episodes: 163\n",
      "5114: reward:  38.00, mean_100:  38.76, episodes: 164\n",
      "5158: reward:  44.00, mean_100:  38.96, episodes: 165\n",
      "5193: reward:  35.00, mean_100:  39.04, episodes: 166\n",
      "5240: reward:  47.00, mean_100:  39.35, episodes: 167\n",
      "5288: reward:  48.00, mean_100:  39.47, episodes: 168\n",
      "5335: reward:  47.00, mean_100:  39.83, episodes: 169\n",
      "5351: reward:  16.00, mean_100:  39.83, episodes: 170\n",
      "5391: reward:  40.00, mean_100:  39.96, episodes: 171\n",
      "5489: reward:  98.00, mean_100:  40.73, episodes: 172\n",
      "5526: reward:  37.00, mean_100:  41.00, episodes: 173\n",
      "5549: reward:  23.00, mean_100:  40.99, episodes: 174\n",
      "5589: reward:  40.00, mean_100:  41.27, episodes: 175\n",
      "5620: reward:  31.00, mean_100:  41.02, episodes: 176\n",
      "5643: reward:  23.00, mean_100:  41.13, episodes: 177\n",
      "5675: reward:  32.00, mean_100:  40.95, episodes: 178\n",
      "5739: reward:  64.00, mean_100:  41.48, episodes: 179\n",
      "5766: reward:  27.00, mean_100:  41.54, episodes: 180\n",
      "5785: reward:  19.00, mean_100:  41.50, episodes: 181\n",
      "5828: reward:  43.00, mean_100:  41.83, episodes: 182\n",
      "5859: reward:  31.00, mean_100:  41.93, episodes: 183\n",
      "5875: reward:  16.00, mean_100:  41.97, episodes: 184\n",
      "5926: reward:  51.00, mean_100:  42.21, episodes: 185\n",
      "5971: reward:  45.00, mean_100:  42.45, episodes: 186\n",
      "5988: reward:  17.00, mean_100:  42.50, episodes: 187\n",
      "6011: reward:  23.00, mean_100:  42.50, episodes: 188\n",
      "6046: reward:  35.00, mean_100:  42.63, episodes: 189\n",
      "6173: reward: 127.00, mean_100:  43.73, episodes: 190\n",
      "6295: reward: 122.00, mean_100:  44.82, episodes: 191\n",
      "6342: reward:  47.00, mean_100:  44.93, episodes: 192\n",
      "6408: reward:  66.00, mean_100:  45.42, episodes: 193\n",
      "6446: reward:  38.00, mean_100:  45.52, episodes: 194\n",
      "6491: reward:  45.00, mean_100:  45.72, episodes: 195\n",
      "6548: reward:  57.00, mean_100:  45.84, episodes: 196\n",
      "6612: reward:  64.00, mean_100:  46.24, episodes: 197\n",
      "6662: reward:  50.00, mean_100:  46.30, episodes: 198\n",
      "6766: reward: 104.00, mean_100:  46.94, episodes: 199\n",
      "6811: reward:  45.00, mean_100:  47.16, episodes: 200\n",
      "6854: reward:  43.00, mean_100:  47.28, episodes: 201\n",
      "6914: reward:  60.00, mean_100:  47.46, episodes: 202\n",
      "6965: reward:  51.00, mean_100:  47.68, episodes: 203\n",
      "7060: reward:  95.00, mean_100:  48.37, episodes: 204\n",
      "7099: reward:  39.00, mean_100:  48.36, episodes: 205\n",
      "7157: reward:  58.00, mean_100:  48.71, episodes: 206\n",
      "7190: reward:  33.00, mean_100:  48.91, episodes: 207\n",
      "7337: reward: 147.00, mean_100:  49.70, episodes: 208\n",
      "7411: reward:  74.00, mean_100:  49.89, episodes: 209\n",
      "7508: reward:  97.00, mean_100:  50.42, episodes: 210\n",
      "7565: reward:  57.00, mean_100:  50.53, episodes: 211\n",
      "7646: reward:  81.00, mean_100:  51.14, episodes: 212\n",
      "7691: reward:  45.00, mean_100:  51.25, episodes: 213\n",
      "7767: reward:  76.00, mean_100:  51.63, episodes: 214\n",
      "7882: reward: 115.00, mean_100:  52.61, episodes: 215\n",
      "7979: reward:  97.00, mean_100:  53.26, episodes: 216\n",
      "8087: reward: 108.00, mean_100:  54.16, episodes: 217\n",
      "8217: reward: 130.00, mean_100:  55.02, episodes: 218\n",
      "8249: reward:  32.00, mean_100:  55.01, episodes: 219\n",
      "8305: reward:  56.00, mean_100:  54.97, episodes: 220\n",
      "8338: reward:  33.00, mean_100:  54.94, episodes: 221\n",
      "8364: reward:  26.00, mean_100:  54.90, episodes: 222\n",
      "8420: reward:  56.00, mean_100:  54.66, episodes: 223\n",
      "8481: reward:  61.00, mean_100:  54.35, episodes: 224\n",
      "8515: reward:  34.00, mean_100:  54.44, episodes: 225\n",
      "8532: reward:  17.00, mean_100:  54.04, episodes: 226\n",
      "8557: reward:  25.00, mean_100:  53.72, episodes: 227\n",
      "8576: reward:  19.00, mean_100:  53.65, episodes: 228\n",
      "8623: reward:  47.00, mean_100:  53.89, episodes: 229\n",
      "8656: reward:  33.00, mean_100:  53.58, episodes: 230\n",
      "8701: reward:  45.00, mean_100:  53.06, episodes: 231\n",
      "8729: reward:  28.00, mean_100:  51.68, episodes: 232\n",
      "8757: reward:  28.00, mean_100:  51.68, episodes: 233\n",
      "8776: reward:  19.00, mean_100:  51.63, episodes: 234\n",
      "8811: reward:  35.00, mean_100:  51.63, episodes: 235\n",
      "8832: reward:  21.00, mean_100:  51.54, episodes: 236\n",
      "8896: reward:  64.00, mean_100:  51.76, episodes: 237\n",
      "8915: reward:  19.00, mean_100:  51.10, episodes: 238\n",
      "8941: reward:  26.00, mean_100:  50.65, episodes: 239\n",
      "8964: reward:  23.00, mean_100:  50.56, episodes: 240\n",
      "9002: reward:  38.00, mean_100:  50.20, episodes: 241\n",
      "9039: reward:  37.00, mean_100:  50.03, episodes: 242\n",
      "9068: reward:  29.00, mean_100:  49.51, episodes: 243\n",
      "9105: reward:  37.00, mean_100:  49.25, episodes: 244\n",
      "9126: reward:  21.00, mean_100:  48.85, episodes: 245\n",
      "9233: reward: 107.00, mean_100:  49.68, episodes: 246\n",
      "9279: reward:  46.00, mean_100:  49.31, episodes: 247\n",
      "9338: reward:  59.00, mean_100:  49.11, episodes: 248\n",
      "9407: reward:  69.00, mean_100:  49.44, episodes: 249\n",
      "9524: reward: 117.00, mean_100:  50.32, episodes: 250\n",
      "9647: reward: 123.00, mean_100:  51.20, episodes: 251\n",
      "9698: reward:  51.00, mean_100:  51.14, episodes: 252\n",
      "9732: reward:  34.00, mean_100:  51.35, episodes: 253\n",
      "9774: reward:  42.00, mean_100:  50.92, episodes: 254\n",
      "9885: reward: 111.00, mean_100:  51.42, episodes: 255\n",
      "9931: reward:  46.00, mean_100:  51.64, episodes: 256\n",
      "9979: reward:  48.00, mean_100:  51.73, episodes: 257\n",
      "10057: reward:  78.00, mean_100:  51.62, episodes: 258\n",
      "10117: reward:  60.00, mean_100:  52.01, episodes: 259\n",
      "10159: reward:  42.00, mean_100:  52.18, episodes: 260\n",
      "10308: reward: 149.00, mean_100:  53.10, episodes: 261\n",
      "10373: reward:  65.00, mean_100:  53.40, episodes: 262\n",
      "10462: reward:  89.00, mean_100:  53.86, episodes: 263\n",
      "10523: reward:  61.00, mean_100:  54.09, episodes: 264\n",
      "10602: reward:  79.00, mean_100:  54.44, episodes: 265\n",
      "10652: reward:  50.00, mean_100:  54.59, episodes: 266\n",
      "10736: reward:  84.00, mean_100:  54.96, episodes: 267\n",
      "10780: reward:  44.00, mean_100:  54.92, episodes: 268\n",
      "10838: reward:  58.00, mean_100:  55.03, episodes: 269\n",
      "10913: reward:  75.00, mean_100:  55.62, episodes: 270\n",
      "10959: reward:  46.00, mean_100:  55.68, episodes: 271\n",
      "11039: reward:  80.00, mean_100:  55.50, episodes: 272\n",
      "11120: reward:  81.00, mean_100:  55.94, episodes: 273\n",
      "11186: reward:  66.00, mean_100:  56.37, episodes: 274\n",
      "11245: reward:  59.00, mean_100:  56.56, episodes: 275\n",
      "11293: reward:  48.00, mean_100:  56.73, episodes: 276\n",
      "11330: reward:  37.00, mean_100:  56.87, episodes: 277\n",
      "11368: reward:  38.00, mean_100:  56.93, episodes: 278\n",
      "11395: reward:  27.00, mean_100:  56.56, episodes: 279\n",
      "11441: reward:  46.00, mean_100:  56.75, episodes: 280\n",
      "11496: reward:  55.00, mean_100:  57.11, episodes: 281\n",
      "11543: reward:  47.00, mean_100:  57.15, episodes: 282\n",
      "11594: reward:  51.00, mean_100:  57.35, episodes: 283\n",
      "11610: reward:  16.00, mean_100:  57.35, episodes: 284\n",
      "11670: reward:  60.00, mean_100:  57.44, episodes: 285\n",
      "11727: reward:  57.00, mean_100:  57.56, episodes: 286\n",
      "11810: reward:  83.00, mean_100:  58.22, episodes: 287\n",
      "11930: reward: 120.00, mean_100:  59.19, episodes: 288\n",
      "11993: reward:  63.00, mean_100:  59.47, episodes: 289\n",
      "12104: reward: 111.00, mean_100:  59.31, episodes: 290\n",
      "12223: reward: 119.00, mean_100:  59.28, episodes: 291\n",
      "12346: reward: 123.00, mean_100:  60.04, episodes: 292\n",
      "12475: reward: 129.00, mean_100:  60.67, episodes: 293\n",
      "12539: reward:  64.00, mean_100:  60.93, episodes: 294\n",
      "12717: reward: 178.00, mean_100:  62.26, episodes: 295\n",
      "12818: reward: 101.00, mean_100:  62.70, episodes: 296\n",
      "13000: reward: 182.00, mean_100:  63.88, episodes: 297\n",
      "13079: reward:  79.00, mean_100:  64.17, episodes: 298\n",
      "13120: reward:  41.00, mean_100:  63.54, episodes: 299\n",
      "13183: reward:  63.00, mean_100:  63.72, episodes: 300\n",
      "13207: reward:  24.00, mean_100:  63.53, episodes: 301\n",
      "13245: reward:  38.00, mean_100:  63.31, episodes: 302\n",
      "13274: reward:  29.00, mean_100:  63.09, episodes: 303\n",
      "13315: reward:  41.00, mean_100:  62.55, episodes: 304\n",
      "13360: reward:  45.00, mean_100:  62.61, episodes: 305\n",
      "13412: reward:  52.00, mean_100:  62.55, episodes: 306\n",
      "13486: reward:  74.00, mean_100:  62.96, episodes: 307\n",
      "13575: reward:  89.00, mean_100:  62.38, episodes: 308\n",
      "13633: reward:  58.00, mean_100:  62.22, episodes: 309\n",
      "13746: reward: 113.00, mean_100:  62.38, episodes: 310\n",
      "13878: reward: 132.00, mean_100:  63.13, episodes: 311\n",
      "13942: reward:  64.00, mean_100:  62.96, episodes: 312\n",
      "13968: reward:  26.00, mean_100:  62.77, episodes: 313\n",
      "14069: reward: 101.00, mean_100:  63.02, episodes: 314\n",
      "14102: reward:  33.00, mean_100:  62.20, episodes: 315\n",
      "14183: reward:  81.00, mean_100:  62.04, episodes: 316\n",
      "14299: reward: 116.00, mean_100:  62.12, episodes: 317\n",
      "14417: reward: 118.00, mean_100:  62.00, episodes: 318\n",
      "14517: reward: 100.00, mean_100:  62.68, episodes: 319\n",
      "14657: reward: 140.00, mean_100:  63.52, episodes: 320\n",
      "14724: reward:  67.00, mean_100:  63.86, episodes: 321\n",
      "14783: reward:  59.00, mean_100:  64.19, episodes: 322\n",
      "14858: reward:  75.00, mean_100:  64.38, episodes: 323\n",
      "14951: reward:  93.00, mean_100:  64.70, episodes: 324\n",
      "14999: reward:  48.00, mean_100:  64.84, episodes: 325\n",
      "15078: reward:  79.00, mean_100:  65.46, episodes: 326\n",
      "15257: reward: 179.00, mean_100:  67.00, episodes: 327\n",
      "15418: reward: 161.00, mean_100:  68.42, episodes: 328\n",
      "15481: reward:  63.00, mean_100:  68.58, episodes: 329\n",
      "15604: reward: 123.00, mean_100:  69.48, episodes: 330\n",
      "15744: reward: 140.00, mean_100:  70.43, episodes: 331\n",
      "15943: reward: 199.00, mean_100:  72.14, episodes: 332\n",
      "16143: reward: 200.00, mean_100:  73.86, episodes: 333\n",
      "16343: reward: 200.00, mean_100:  75.67, episodes: 334\n",
      "16543: reward: 200.00, mean_100:  77.32, episodes: 335\n",
      "16743: reward: 200.00, mean_100:  79.11, episodes: 336\n",
      "16943: reward: 200.00, mean_100:  80.47, episodes: 337\n",
      "17113: reward: 170.00, mean_100:  81.98, episodes: 338\n",
      "17260: reward: 147.00, mean_100:  83.19, episodes: 339\n",
      "17316: reward:  56.00, mean_100:  83.52, episodes: 340\n",
      "17516: reward: 200.00, mean_100:  85.14, episodes: 341\n",
      "17712: reward: 196.00, mean_100:  86.73, episodes: 342\n",
      "17898: reward: 186.00, mean_100:  88.30, episodes: 343\n",
      "17922: reward:  24.00, mean_100:  88.17, episodes: 344\n",
      "18044: reward: 122.00, mean_100:  89.18, episodes: 345\n",
      "18114: reward:  70.00, mean_100:  88.81, episodes: 346\n",
      "18217: reward: 103.00, mean_100:  89.38, episodes: 347\n",
      "18276: reward:  59.00, mean_100:  89.38, episodes: 348\n",
      "18348: reward:  72.00, mean_100:  89.41, episodes: 349\n",
      "18418: reward:  70.00, mean_100:  88.94, episodes: 350\n",
      "18514: reward:  96.00, mean_100:  88.67, episodes: 351\n",
      "18612: reward:  98.00, mean_100:  89.14, episodes: 352\n",
      "18668: reward:  56.00, mean_100:  89.36, episodes: 353\n",
      "18770: reward: 102.00, mean_100:  89.96, episodes: 354\n",
      "18850: reward:  80.00, mean_100:  89.65, episodes: 355\n",
      "18955: reward: 105.00, mean_100:  90.24, episodes: 356\n",
      "18970: reward:  15.00, mean_100:  89.91, episodes: 357\n",
      "19003: reward:  33.00, mean_100:  89.46, episodes: 358\n",
      "19089: reward:  86.00, mean_100:  89.72, episodes: 359\n",
      "19190: reward: 101.00, mean_100:  90.31, episodes: 360\n",
      "19280: reward:  90.00, mean_100:  89.72, episodes: 361\n",
      "19308: reward:  28.00, mean_100:  89.35, episodes: 362\n",
      "19441: reward: 133.00, mean_100:  89.79, episodes: 363\n",
      "19623: reward: 182.00, mean_100:  91.00, episodes: 364\n",
      "19641: reward:  18.00, mean_100:  90.39, episodes: 365\n",
      "19678: reward:  37.00, mean_100:  90.26, episodes: 366\n",
      "19811: reward: 133.00, mean_100:  90.75, episodes: 367\n",
      "19952: reward: 141.00, mean_100:  91.72, episodes: 368\n",
      "19968: reward:  16.00, mean_100:  91.30, episodes: 369\n",
      "20010: reward:  42.00, mean_100:  90.97, episodes: 370\n",
      "20095: reward:  85.00, mean_100:  91.36, episodes: 371\n",
      "20247: reward: 152.00, mean_100:  92.08, episodes: 372\n",
      "20368: reward: 121.00, mean_100:  92.48, episodes: 373\n",
      "20568: reward: 200.00, mean_100:  93.82, episodes: 374\n",
      "20705: reward: 137.00, mean_100:  94.60, episodes: 375\n",
      "20867: reward: 162.00, mean_100:  95.74, episodes: 376\n",
      "20887: reward:  20.00, mean_100:  95.57, episodes: 377\n",
      "21061: reward: 174.00, mean_100:  96.93, episodes: 378\n",
      "21181: reward: 120.00, mean_100:  97.86, episodes: 379\n",
      "21317: reward: 136.00, mean_100:  98.76, episodes: 380\n",
      "21480: reward: 163.00, mean_100:  99.84, episodes: 381\n",
      "21680: reward: 200.00, mean_100: 101.37, episodes: 382\n",
      "21866: reward: 186.00, mean_100: 102.72, episodes: 383\n",
      "22064: reward: 198.00, mean_100: 104.54, episodes: 384\n",
      "22216: reward: 152.00, mean_100: 105.46, episodes: 385\n",
      "22406: reward: 190.00, mean_100: 106.79, episodes: 386\n",
      "22606: reward: 200.00, mean_100: 107.96, episodes: 387\n",
      "22806: reward: 200.00, mean_100: 108.76, episodes: 388\n",
      "22970: reward: 164.00, mean_100: 109.77, episodes: 389\n",
      "23097: reward: 127.00, mean_100: 109.93, episodes: 390\n",
      "23126: reward:  29.00, mean_100: 109.03, episodes: 391\n",
      "23307: reward: 181.00, mean_100: 109.61, episodes: 392\n",
      "23507: reward: 200.00, mean_100: 110.32, episodes: 393\n",
      "23647: reward: 140.00, mean_100: 111.08, episodes: 394\n",
      "23847: reward: 200.00, mean_100: 111.30, episodes: 395\n",
      "24007: reward: 160.00, mean_100: 111.89, episodes: 396\n",
      "24100: reward:  93.00, mean_100: 111.00, episodes: 397\n",
      "24285: reward: 185.00, mean_100: 112.06, episodes: 398\n",
      "24485: reward: 200.00, mean_100: 113.65, episodes: 399\n",
      "24685: reward: 200.00, mean_100: 115.02, episodes: 400\n",
      "24885: reward: 200.00, mean_100: 116.78, episodes: 401\n",
      "25081: reward: 196.00, mean_100: 118.36, episodes: 402\n",
      "25248: reward: 167.00, mean_100: 119.74, episodes: 403\n",
      "25448: reward: 200.00, mean_100: 121.33, episodes: 404\n",
      "25648: reward: 200.00, mean_100: 122.88, episodes: 405\n",
      "25848: reward: 200.00, mean_100: 124.36, episodes: 406\n",
      "25945: reward:  97.00, mean_100: 124.59, episodes: 407\n",
      "26066: reward: 121.00, mean_100: 124.91, episodes: 408\n",
      "26266: reward: 200.00, mean_100: 126.33, episodes: 409\n",
      "26466: reward: 200.00, mean_100: 127.20, episodes: 410\n",
      "26495: reward:  29.00, mean_100: 126.17, episodes: 411\n",
      "26529: reward:  34.00, mean_100: 125.87, episodes: 412\n",
      "26729: reward: 200.00, mean_100: 127.61, episodes: 413\n",
      "26929: reward: 200.00, mean_100: 128.60, episodes: 414\n",
      "27129: reward: 200.00, mean_100: 130.27, episodes: 415\n",
      "27329: reward: 200.00, mean_100: 131.46, episodes: 416\n",
      "27494: reward: 165.00, mean_100: 131.95, episodes: 417\n",
      "27663: reward: 169.00, mean_100: 132.46, episodes: 418\n",
      "27774: reward: 111.00, mean_100: 132.57, episodes: 419\n",
      "27893: reward: 119.00, mean_100: 132.36, episodes: 420\n",
      "28005: reward: 112.00, mean_100: 132.81, episodes: 421\n",
      "28108: reward: 103.00, mean_100: 133.25, episodes: 422\n",
      "28231: reward: 123.00, mean_100: 133.73, episodes: 423\n",
      "28254: reward:  23.00, mean_100: 133.03, episodes: 424\n",
      "28377: reward: 123.00, mean_100: 133.78, episodes: 425\n",
      "28434: reward:  57.00, mean_100: 133.56, episodes: 426\n",
      "28462: reward:  28.00, mean_100: 132.05, episodes: 427\n",
      "28497: reward:  35.00, mean_100: 130.79, episodes: 428\n",
      "28541: reward:  44.00, mean_100: 130.60, episodes: 429\n",
      "28642: reward: 101.00, mean_100: 130.38, episodes: 430\n",
      "28678: reward:  36.00, mean_100: 129.34, episodes: 431\n",
      "28716: reward:  38.00, mean_100: 127.73, episodes: 432\n",
      "28752: reward:  36.00, mean_100: 126.09, episodes: 433\n",
      "28806: reward:  54.00, mean_100: 124.63, episodes: 434\n",
      "28832: reward:  26.00, mean_100: 122.89, episodes: 435\n",
      "28856: reward:  24.00, mean_100: 121.13, episodes: 436\n",
      "28915: reward:  59.00, mean_100: 119.72, episodes: 437\n",
      "29012: reward:  97.00, mean_100: 118.99, episodes: 438\n",
      "29037: reward:  25.00, mean_100: 117.77, episodes: 439\n",
      "29089: reward:  52.00, mean_100: 117.73, episodes: 440\n",
      "29151: reward:  62.00, mean_100: 116.35, episodes: 441\n",
      "29193: reward:  42.00, mean_100: 114.81, episodes: 442\n",
      "29264: reward:  71.00, mean_100: 113.66, episodes: 443\n",
      "29326: reward:  62.00, mean_100: 114.04, episodes: 444\n",
      "29411: reward:  85.00, mean_100: 113.67, episodes: 445\n",
      "29455: reward:  44.00, mean_100: 113.41, episodes: 446\n",
      "29542: reward:  87.00, mean_100: 113.25, episodes: 447\n",
      "29659: reward: 117.00, mean_100: 113.83, episodes: 448\n",
      "29809: reward: 150.00, mean_100: 114.61, episodes: 449\n",
      "29897: reward:  88.00, mean_100: 114.79, episodes: 450\n",
      "30002: reward: 105.00, mean_100: 114.88, episodes: 451\n",
      "30044: reward:  42.00, mean_100: 114.32, episodes: 452\n",
      "30118: reward:  74.00, mean_100: 114.50, episodes: 453\n",
      "30189: reward:  71.00, mean_100: 114.19, episodes: 454\n",
      "30244: reward:  55.00, mean_100: 113.94, episodes: 455\n",
      "30276: reward:  32.00, mean_100: 113.21, episodes: 456\n",
      "30333: reward:  57.00, mean_100: 113.63, episodes: 457\n",
      "30381: reward:  48.00, mean_100: 113.78, episodes: 458\n",
      "30457: reward:  76.00, mean_100: 113.68, episodes: 459\n",
      "30498: reward:  41.00, mean_100: 113.08, episodes: 460\n",
      "30535: reward:  37.00, mean_100: 112.55, episodes: 461\n",
      "30560: reward:  25.00, mean_100: 112.52, episodes: 462\n",
      "30581: reward:  21.00, mean_100: 111.40, episodes: 463\n",
      "30627: reward:  46.00, mean_100: 110.04, episodes: 464\n",
      "30694: reward:  67.00, mean_100: 110.53, episodes: 465\n",
      "30737: reward:  43.00, mean_100: 110.59, episodes: 466\n",
      "30781: reward:  44.00, mean_100: 109.70, episodes: 467\n",
      "30820: reward:  39.00, mean_100: 108.68, episodes: 468\n",
      "30900: reward:  80.00, mean_100: 109.32, episodes: 469\n",
      "30974: reward:  74.00, mean_100: 109.64, episodes: 470\n",
      "31029: reward:  55.00, mean_100: 109.34, episodes: 471\n",
      "31093: reward:  64.00, mean_100: 108.46, episodes: 472\n",
      "31179: reward:  86.00, mean_100: 108.11, episodes: 473\n",
      "31307: reward: 128.00, mean_100: 107.39, episodes: 474\n",
      "31332: reward:  25.00, mean_100: 106.27, episodes: 475\n",
      "31380: reward:  48.00, mean_100: 105.13, episodes: 476\n",
      "31473: reward:  93.00, mean_100: 105.86, episodes: 477\n",
      "31529: reward:  56.00, mean_100: 104.68, episodes: 478\n",
      "31558: reward:  29.00, mean_100: 103.77, episodes: 479\n",
      "31647: reward:  89.00, mean_100: 103.30, episodes: 480\n",
      "31747: reward: 100.00, mean_100: 102.67, episodes: 481\n",
      "31856: reward: 109.00, mean_100: 101.76, episodes: 482\n",
      "31967: reward: 111.00, mean_100: 101.01, episodes: 483\n",
      "32087: reward: 120.00, mean_100: 100.23, episodes: 484\n",
      "32216: reward: 129.00, mean_100: 100.00, episodes: 485\n",
      "32353: reward: 137.00, mean_100:  99.47, episodes: 486\n",
      "32453: reward: 100.00, mean_100:  98.47, episodes: 487\n",
      "32600: reward: 147.00, mean_100:  97.94, episodes: 488\n",
      "32750: reward: 150.00, mean_100:  97.80, episodes: 489\n",
      "32910: reward: 160.00, mean_100:  98.13, episodes: 490\n",
      "33022: reward: 112.00, mean_100:  98.96, episodes: 491\n",
      "33133: reward: 111.00, mean_100:  98.26, episodes: 492\n",
      "33246: reward: 113.00, mean_100:  97.39, episodes: 493\n",
      "33315: reward:  69.00, mean_100:  96.68, episodes: 494\n",
      "33433: reward: 118.00, mean_100:  95.86, episodes: 495\n",
      "33547: reward: 114.00, mean_100:  95.40, episodes: 496\n",
      "33688: reward: 141.00, mean_100:  95.88, episodes: 497\n",
      "33799: reward: 111.00, mean_100:  95.14, episodes: 498\n",
      "33904: reward: 105.00, mean_100:  94.19, episodes: 499\n",
      "34038: reward: 134.00, mean_100:  93.53, episodes: 500\n",
      "34150: reward: 112.00, mean_100:  92.65, episodes: 501\n",
      "34267: reward: 117.00, mean_100:  91.86, episodes: 502\n",
      "34408: reward: 141.00, mean_100:  91.60, episodes: 503\n",
      "34608: reward: 200.00, mean_100:  91.60, episodes: 504\n",
      "34808: reward: 200.00, mean_100:  91.60, episodes: 505\n",
      "34931: reward: 123.00, mean_100:  90.83, episodes: 506\n",
      "35105: reward: 174.00, mean_100:  91.60, episodes: 507\n",
      "35241: reward: 136.00, mean_100:  91.75, episodes: 508\n",
      "35352: reward: 111.00, mean_100:  90.86, episodes: 509\n",
      "35512: reward: 160.00, mean_100:  90.46, episodes: 510\n",
      "35712: reward: 200.00, mean_100:  92.17, episodes: 511\n",
      "35912: reward: 200.00, mean_100:  93.83, episodes: 512\n",
      "36092: reward: 180.00, mean_100:  93.63, episodes: 513\n",
      "36292: reward: 200.00, mean_100:  93.63, episodes: 514\n",
      "36492: reward: 200.00, mean_100:  93.63, episodes: 515\n",
      "36692: reward: 200.00, mean_100:  93.63, episodes: 516\n",
      "36892: reward: 200.00, mean_100:  93.98, episodes: 517\n",
      "37092: reward: 200.00, mean_100:  94.29, episodes: 518\n",
      "37292: reward: 200.00, mean_100:  95.18, episodes: 519\n",
      "37492: reward: 200.00, mean_100:  95.99, episodes: 520\n",
      "37692: reward: 200.00, mean_100:  96.87, episodes: 521\n",
      "37892: reward: 200.00, mean_100:  97.84, episodes: 522\n",
      "38092: reward: 200.00, mean_100:  98.61, episodes: 523\n",
      "38292: reward: 200.00, mean_100: 100.38, episodes: 524\n",
      "38492: reward: 200.00, mean_100: 101.15, episodes: 525\n",
      "38692: reward: 200.00, mean_100: 102.58, episodes: 526\n",
      "38831: reward: 139.00, mean_100: 103.69, episodes: 527\n",
      "38995: reward: 164.00, mean_100: 104.98, episodes: 528\n",
      "39180: reward: 185.00, mean_100: 106.39, episodes: 529\n",
      "39320: reward: 140.00, mean_100: 106.78, episodes: 530\n",
      "39466: reward: 146.00, mean_100: 107.88, episodes: 531\n",
      "39585: reward: 119.00, mean_100: 108.69, episodes: 532\n",
      "39716: reward: 131.00, mean_100: 109.64, episodes: 533\n",
      "39863: reward: 147.00, mean_100: 110.57, episodes: 534\n",
      "40063: reward: 200.00, mean_100: 112.31, episodes: 535\n",
      "40263: reward: 200.00, mean_100: 114.07, episodes: 536\n",
      "40438: reward: 175.00, mean_100: 115.23, episodes: 537\n",
      "40638: reward: 200.00, mean_100: 116.26, episodes: 538\n",
      "40801: reward: 163.00, mean_100: 117.64, episodes: 539\n",
      "41001: reward: 200.00, mean_100: 119.12, episodes: 540\n",
      "41174: reward: 173.00, mean_100: 120.23, episodes: 541\n",
      "41372: reward: 198.00, mean_100: 121.79, episodes: 542\n",
      "41572: reward: 200.00, mean_100: 123.08, episodes: 543\n",
      "41772: reward: 200.00, mean_100: 124.46, episodes: 544\n",
      "41960: reward: 188.00, mean_100: 125.49, episodes: 545\n",
      "42149: reward: 189.00, mean_100: 126.94, episodes: 546\n",
      "42349: reward: 200.00, mean_100: 128.07, episodes: 547\n",
      "42490: reward: 141.00, mean_100: 128.31, episodes: 548\n",
      "42644: reward: 154.00, mean_100: 128.35, episodes: 549\n",
      "42792: reward: 148.00, mean_100: 128.95, episodes: 550\n",
      "42963: reward: 171.00, mean_100: 129.61, episodes: 551\n",
      "43132: reward: 169.00, mean_100: 130.88, episodes: 552\n",
      "43301: reward: 169.00, mean_100: 131.83, episodes: 553\n",
      "43480: reward: 179.00, mean_100: 132.91, episodes: 554\n",
      "43651: reward: 171.00, mean_100: 134.07, episodes: 555\n",
      "43788: reward: 137.00, mean_100: 135.12, episodes: 556\n",
      "43931: reward: 143.00, mean_100: 135.98, episodes: 557\n",
      "44097: reward: 166.00, mean_100: 137.16, episodes: 558\n",
      "44263: reward: 166.00, mean_100: 138.06, episodes: 559\n",
      "44391: reward: 128.00, mean_100: 138.93, episodes: 560\n",
      "44442: reward:  51.00, mean_100: 139.07, episodes: 561\n",
      "44578: reward: 136.00, mean_100: 140.18, episodes: 562\n",
      "44711: reward: 133.00, mean_100: 141.30, episodes: 563\n",
      "44851: reward: 140.00, mean_100: 142.24, episodes: 564\n",
      "44967: reward: 116.00, mean_100: 142.73, episodes: 565\n",
      "45084: reward: 117.00, mean_100: 143.47, episodes: 566\n",
      "45204: reward: 120.00, mean_100: 144.23, episodes: 567\n",
      "45314: reward: 110.00, mean_100: 144.94, episodes: 568\n",
      "45382: reward:  68.00, mean_100: 144.82, episodes: 569\n",
      "45461: reward:  79.00, mean_100: 144.87, episodes: 570\n",
      "45588: reward: 127.00, mean_100: 145.59, episodes: 571\n",
      "45631: reward:  43.00, mean_100: 145.38, episodes: 572\n",
      "45664: reward:  33.00, mean_100: 144.85, episodes: 573\n",
      "45762: reward:  98.00, mean_100: 144.55, episodes: 574\n",
      "45870: reward: 108.00, mean_100: 145.38, episodes: 575\n",
      "45952: reward:  82.00, mean_100: 145.72, episodes: 576\n",
      "46060: reward: 108.00, mean_100: 145.87, episodes: 577\n",
      "46101: reward:  41.00, mean_100: 145.72, episodes: 578\n",
      "46142: reward:  41.00, mean_100: 145.84, episodes: 579\n",
      "46172: reward:  30.00, mean_100: 145.25, episodes: 580\n",
      "46237: reward:  65.00, mean_100: 144.90, episodes: 581\n",
      "46269: reward:  32.00, mean_100: 144.13, episodes: 582\n",
      "46306: reward:  37.00, mean_100: 143.39, episodes: 583\n",
      "46352: reward:  46.00, mean_100: 142.65, episodes: 584\n",
      "46449: reward:  97.00, mean_100: 142.33, episodes: 585\n",
      "46533: reward:  84.00, mean_100: 141.80, episodes: 586\n",
      "46618: reward:  85.00, mean_100: 141.65, episodes: 587\n",
      "46653: reward:  35.00, mean_100: 140.53, episodes: 588\n",
      "46684: reward:  31.00, mean_100: 139.34, episodes: 589\n",
      "46756: reward:  72.00, mean_100: 138.46, episodes: 590\n",
      "46873: reward: 117.00, mean_100: 138.51, episodes: 591\n",
      "46908: reward:  35.00, mean_100: 137.75, episodes: 592\n",
      "46962: reward:  54.00, mean_100: 137.16, episodes: 593\n",
      "47076: reward: 114.00, mean_100: 137.61, episodes: 594\n",
      "47146: reward:  70.00, mean_100: 137.13, episodes: 595\n",
      "47216: reward:  70.00, mean_100: 136.69, episodes: 596\n",
      "47261: reward:  45.00, mean_100: 135.73, episodes: 597\n",
      "47356: reward:  95.00, mean_100: 135.57, episodes: 598\n",
      "47469: reward: 113.00, mean_100: 135.65, episodes: 599\n",
      "47558: reward:  89.00, mean_100: 135.20, episodes: 600\n",
      "47631: reward:  73.00, mean_100: 134.81, episodes: 601\n",
      "47727: reward:  96.00, mean_100: 134.60, episodes: 602\n",
      "47822: reward:  95.00, mean_100: 134.14, episodes: 603\n",
      "47916: reward:  94.00, mean_100: 133.08, episodes: 604\n",
      "48036: reward: 120.00, mean_100: 132.28, episodes: 605\n",
      "48142: reward: 106.00, mean_100: 132.11, episodes: 606\n",
      "48276: reward: 134.00, mean_100: 131.71, episodes: 607\n",
      "48374: reward:  98.00, mean_100: 131.33, episodes: 608\n",
      "48506: reward: 132.00, mean_100: 131.54, episodes: 609\n",
      "48653: reward: 147.00, mean_100: 131.41, episodes: 610\n",
      "48772: reward: 119.00, mean_100: 130.60, episodes: 611\n",
      "48882: reward: 110.00, mean_100: 129.70, episodes: 612\n",
      "48992: reward: 110.00, mean_100: 129.00, episodes: 613\n",
      "49110: reward: 118.00, mean_100: 128.18, episodes: 614\n",
      "49227: reward: 117.00, mean_100: 127.35, episodes: 615\n",
      "49361: reward: 134.00, mean_100: 126.69, episodes: 616\n",
      "49471: reward: 110.00, mean_100: 125.79, episodes: 617\n",
      "49565: reward:  94.00, mean_100: 124.73, episodes: 618\n",
      "49687: reward: 122.00, mean_100: 123.95, episodes: 619\n",
      "49810: reward: 123.00, mean_100: 123.18, episodes: 620\n",
      "49925: reward: 115.00, mean_100: 122.33, episodes: 621\n",
      "50003: reward:  78.00, mean_100: 121.11, episodes: 622\n",
      "50111: reward: 108.00, mean_100: 120.19, episodes: 623\n",
      "50216: reward: 105.00, mean_100: 119.24, episodes: 624\n",
      "50326: reward: 110.00, mean_100: 118.34, episodes: 625\n",
      "50431: reward: 105.00, mean_100: 117.39, episodes: 626\n",
      "50536: reward: 105.00, mean_100: 117.05, episodes: 627\n",
      "50651: reward: 115.00, mean_100: 116.56, episodes: 628\n",
      "50782: reward: 131.00, mean_100: 116.02, episodes: 629\n",
      "50899: reward: 117.00, mean_100: 115.79, episodes: 630\n",
      "51011: reward: 112.00, mean_100: 115.45, episodes: 631\n",
      "51093: reward:  82.00, mean_100: 115.08, episodes: 632\n",
      "51190: reward:  97.00, mean_100: 114.74, episodes: 633\n",
      "51293: reward: 103.00, mean_100: 114.30, episodes: 634\n",
      "51336: reward:  43.00, mean_100: 112.73, episodes: 635\n",
      "51423: reward:  87.00, mean_100: 111.60, episodes: 636\n",
      "51516: reward:  93.00, mean_100: 110.78, episodes: 637\n",
      "51568: reward:  52.00, mean_100: 109.30, episodes: 638\n",
      "51650: reward:  82.00, mean_100: 108.49, episodes: 639\n",
      "51737: reward:  87.00, mean_100: 107.36, episodes: 640\n",
      "51780: reward:  43.00, mean_100: 106.06, episodes: 641\n",
      "51877: reward:  97.00, mean_100: 105.05, episodes: 642\n",
      "51913: reward:  36.00, mean_100: 103.41, episodes: 643\n",
      "51937: reward:  24.00, mean_100: 101.65, episodes: 644\n",
      "51978: reward:  41.00, mean_100: 100.18, episodes: 645\n",
      "52079: reward: 101.00, mean_100:  99.30, episodes: 646\n",
      "52181: reward: 102.00, mean_100:  98.32, episodes: 647\n",
      "52282: reward: 101.00, mean_100:  97.92, episodes: 648\n",
      "52391: reward: 109.00, mean_100:  97.47, episodes: 649\n",
      "52508: reward: 117.00, mean_100:  97.16, episodes: 650\n",
      "52613: reward: 105.00, mean_100:  96.50, episodes: 651\n",
      "52723: reward: 110.00, mean_100:  95.91, episodes: 652\n",
      "52833: reward: 110.00, mean_100:  95.32, episodes: 653\n",
      "52921: reward:  88.00, mean_100:  94.41, episodes: 654\n",
      "52972: reward:  51.00, mean_100:  93.21, episodes: 655\n",
      "53052: reward:  80.00, mean_100:  92.64, episodes: 656\n",
      "53132: reward:  80.00, mean_100:  92.01, episodes: 657\n",
      "53220: reward:  88.00, mean_100:  91.23, episodes: 658\n",
      "53255: reward:  35.00, mean_100:  89.92, episodes: 659\n",
      "53351: reward:  96.00, mean_100:  89.60, episodes: 660\n",
      "53416: reward:  65.00, mean_100:  89.74, episodes: 661\n",
      "53451: reward:  35.00, mean_100:  88.73, episodes: 662\n",
      "53532: reward:  81.00, mean_100:  88.21, episodes: 663\n",
      "53613: reward:  81.00, mean_100:  87.62, episodes: 664\n",
      "53702: reward:  89.00, mean_100:  87.35, episodes: 665\n",
      "53805: reward: 103.00, mean_100:  87.21, episodes: 666\n",
      "53891: reward:  86.00, mean_100:  86.87, episodes: 667\n",
      "53981: reward:  90.00, mean_100:  86.67, episodes: 668\n",
      "54077: reward:  96.00, mean_100:  86.95, episodes: 669\n",
      "54172: reward:  95.00, mean_100:  87.11, episodes: 670\n",
      "54263: reward:  91.00, mean_100:  86.75, episodes: 671\n",
      "54379: reward: 116.00, mean_100:  87.48, episodes: 672\n",
      "54488: reward: 109.00, mean_100:  88.24, episodes: 673\n",
      "54619: reward: 131.00, mean_100:  88.57, episodes: 674\n",
      "54736: reward: 117.00, mean_100:  88.66, episodes: 675\n",
      "54849: reward: 113.00, mean_100:  88.97, episodes: 676\n",
      "54971: reward: 122.00, mean_100:  89.11, episodes: 677\n",
      "55082: reward: 111.00, mean_100:  89.81, episodes: 678\n",
      "55203: reward: 121.00, mean_100:  90.61, episodes: 679\n",
      "55328: reward: 125.00, mean_100:  91.56, episodes: 680\n",
      "55433: reward: 105.00, mean_100:  91.96, episodes: 681\n",
      "55473: reward:  40.00, mean_100:  92.04, episodes: 682\n",
      "55580: reward: 107.00, mean_100:  92.74, episodes: 683\n",
      "55702: reward: 122.00, mean_100:  93.50, episodes: 684\n",
      "55821: reward: 119.00, mean_100:  93.72, episodes: 685\n",
      "55914: reward:  93.00, mean_100:  93.81, episodes: 686\n",
      "55992: reward:  78.00, mean_100:  93.74, episodes: 687\n",
      "56089: reward:  97.00, mean_100:  94.36, episodes: 688\n",
      "56175: reward:  86.00, mean_100:  94.91, episodes: 689\n",
      "56280: reward: 105.00, mean_100:  95.24, episodes: 690\n",
      "56386: reward: 106.00, mean_100:  95.13, episodes: 691\n",
      "56507: reward: 121.00, mean_100:  95.99, episodes: 692\n",
      "56621: reward: 114.00, mean_100:  96.59, episodes: 693\n",
      "56741: reward: 120.00, mean_100:  96.65, episodes: 694\n",
      "56865: reward: 124.00, mean_100:  97.19, episodes: 695\n",
      "57008: reward: 143.00, mean_100:  97.92, episodes: 696\n",
      "57148: reward: 140.00, mean_100:  98.87, episodes: 697\n",
      "57293: reward: 145.00, mean_100:  99.37, episodes: 698\n",
      "57462: reward: 169.00, mean_100:  99.93, episodes: 699\n",
      "57613: reward: 151.00, mean_100: 100.55, episodes: 700\n",
      "57813: reward: 200.00, mean_100: 101.82, episodes: 701\n",
      "58013: reward: 200.00, mean_100: 102.86, episodes: 702\n",
      "58213: reward: 200.00, mean_100: 103.91, episodes: 703\n",
      "58413: reward: 200.00, mean_100: 104.97, episodes: 704\n",
      "58613: reward: 200.00, mean_100: 105.77, episodes: 705\n",
      "58813: reward: 200.00, mean_100: 106.71, episodes: 706\n",
      "59013: reward: 200.00, mean_100: 107.37, episodes: 707\n",
      "59213: reward: 200.00, mean_100: 108.39, episodes: 708\n",
      "59413: reward: 200.00, mean_100: 109.07, episodes: 709\n",
      "59613: reward: 200.00, mean_100: 109.60, episodes: 710\n",
      "59813: reward: 200.00, mean_100: 110.41, episodes: 711\n",
      "60013: reward: 200.00, mean_100: 111.31, episodes: 712\n",
      "60213: reward: 200.00, mean_100: 112.21, episodes: 713\n",
      "60413: reward: 200.00, mean_100: 113.03, episodes: 714\n",
      "60613: reward: 200.00, mean_100: 113.86, episodes: 715\n",
      "60813: reward: 200.00, mean_100: 114.52, episodes: 716\n",
      "61013: reward: 200.00, mean_100: 115.42, episodes: 717\n",
      "61213: reward: 200.00, mean_100: 116.48, episodes: 718\n",
      "61413: reward: 200.00, mean_100: 117.26, episodes: 719\n",
      "61613: reward: 200.00, mean_100: 118.03, episodes: 720\n",
      "61813: reward: 200.00, mean_100: 118.88, episodes: 721\n",
      "62013: reward: 200.00, mean_100: 120.10, episodes: 722\n",
      "62213: reward: 200.00, mean_100: 121.02, episodes: 723\n",
      "62407: reward: 194.00, mean_100: 121.91, episodes: 724\n",
      "62607: reward: 200.00, mean_100: 122.81, episodes: 725\n",
      "62793: reward: 186.00, mean_100: 123.62, episodes: 726\n",
      "62993: reward: 200.00, mean_100: 124.57, episodes: 727\n",
      "63190: reward: 197.00, mean_100: 125.39, episodes: 728\n",
      "63390: reward: 200.00, mean_100: 126.08, episodes: 729\n",
      "63590: reward: 200.00, mean_100: 126.91, episodes: 730\n",
      "63790: reward: 200.00, mean_100: 127.79, episodes: 731\n",
      "63990: reward: 200.00, mean_100: 128.97, episodes: 732\n",
      "64190: reward: 200.00, mean_100: 130.00, episodes: 733\n",
      "64372: reward: 182.00, mean_100: 130.79, episodes: 734\n",
      "64557: reward: 185.00, mean_100: 132.21, episodes: 735\n",
      "64746: reward: 189.00, mean_100: 133.23, episodes: 736\n",
      "64918: reward: 172.00, mean_100: 134.02, episodes: 737\n",
      "65067: reward: 149.00, mean_100: 134.99, episodes: 738\n",
      "65231: reward: 164.00, mean_100: 135.81, episodes: 739\n",
      "65404: reward: 173.00, mean_100: 136.67, episodes: 740\n",
      "65589: reward: 185.00, mean_100: 138.09, episodes: 741\n",
      "65784: reward: 195.00, mean_100: 139.07, episodes: 742\n",
      "65956: reward: 172.00, mean_100: 140.43, episodes: 743\n",
      "66132: reward: 176.00, mean_100: 141.95, episodes: 744\n",
      "66297: reward: 165.00, mean_100: 143.19, episodes: 745\n",
      "66457: reward: 160.00, mean_100: 143.78, episodes: 746\n",
      "66617: reward: 160.00, mean_100: 144.36, episodes: 747\n",
      "66771: reward: 154.00, mean_100: 144.89, episodes: 748\n",
      "66922: reward: 151.00, mean_100: 145.31, episodes: 749\n",
      "67059: reward: 137.00, mean_100: 145.51, episodes: 750\n",
      "67178: reward: 119.00, mean_100: 145.65, episodes: 751\n",
      "67311: reward: 133.00, mean_100: 145.88, episodes: 752\n",
      "67424: reward: 113.00, mean_100: 145.91, episodes: 753\n",
      "67549: reward: 125.00, mean_100: 146.28, episodes: 754\n",
      "67677: reward: 128.00, mean_100: 147.05, episodes: 755\n",
      "67815: reward: 138.00, mean_100: 147.63, episodes: 756\n",
      "67959: reward: 144.00, mean_100: 148.27, episodes: 757\n",
      "68084: reward: 125.00, mean_100: 148.64, episodes: 758\n",
      "68205: reward: 121.00, mean_100: 149.50, episodes: 759\n",
      "68343: reward: 138.00, mean_100: 149.92, episodes: 760\n",
      "68482: reward: 139.00, mean_100: 150.66, episodes: 761\n",
      "68592: reward: 110.00, mean_100: 151.41, episodes: 762\n",
      "68713: reward: 121.00, mean_100: 151.81, episodes: 763\n",
      "68837: reward: 124.00, mean_100: 152.24, episodes: 764\n",
      "68941: reward: 104.00, mean_100: 152.39, episodes: 765\n",
      "69051: reward: 110.00, mean_100: 152.46, episodes: 766\n",
      "69095: reward:  44.00, mean_100: 152.04, episodes: 767\n",
      "69190: reward:  95.00, mean_100: 152.09, episodes: 768\n",
      "69305: reward: 115.00, mean_100: 152.28, episodes: 769\n",
      "69410: reward: 105.00, mean_100: 152.38, episodes: 770\n",
      "69514: reward: 104.00, mean_100: 152.51, episodes: 771\n",
      "69623: reward: 109.00, mean_100: 152.44, episodes: 772\n",
      "69736: reward: 113.00, mean_100: 152.48, episodes: 773\n",
      "69847: reward: 111.00, mean_100: 152.28, episodes: 774\n",
      "69975: reward: 128.00, mean_100: 152.39, episodes: 775\n",
      "70088: reward: 113.00, mean_100: 152.39, episodes: 776\n",
      "70207: reward: 119.00, mean_100: 152.36, episodes: 777\n",
      "70334: reward: 127.00, mean_100: 152.52, episodes: 778\n",
      "70449: reward: 115.00, mean_100: 152.46, episodes: 779\n",
      "70567: reward: 118.00, mean_100: 152.39, episodes: 780\n",
      "70687: reward: 120.00, mean_100: 152.54, episodes: 781\n",
      "70800: reward: 113.00, mean_100: 153.27, episodes: 782\n",
      "70918: reward: 118.00, mean_100: 153.38, episodes: 783\n",
      "71033: reward: 115.00, mean_100: 153.31, episodes: 784\n",
      "71167: reward: 134.00, mean_100: 153.46, episodes: 785\n",
      "71287: reward: 120.00, mean_100: 153.73, episodes: 786\n",
      "71422: reward: 135.00, mean_100: 154.30, episodes: 787\n",
      "71561: reward: 139.00, mean_100: 154.72, episodes: 788\n",
      "71672: reward: 111.00, mean_100: 154.97, episodes: 789\n",
      "71792: reward: 120.00, mean_100: 155.12, episodes: 790\n",
      "71907: reward: 115.00, mean_100: 155.21, episodes: 791\n",
      "72036: reward: 129.00, mean_100: 155.29, episodes: 792\n",
      "72171: reward: 135.00, mean_100: 155.50, episodes: 793\n",
      "72312: reward: 141.00, mean_100: 155.71, episodes: 794\n",
      "72438: reward: 126.00, mean_100: 155.73, episodes: 795\n",
      "72568: reward: 130.00, mean_100: 155.60, episodes: 796\n",
      "72687: reward: 119.00, mean_100: 155.39, episodes: 797\n",
      "72809: reward: 122.00, mean_100: 155.16, episodes: 798\n",
      "72933: reward: 124.00, mean_100: 154.71, episodes: 799\n",
      "73044: reward: 111.00, mean_100: 154.31, episodes: 800\n",
      "73178: reward: 134.00, mean_100: 153.65, episodes: 801\n",
      "73318: reward: 140.00, mean_100: 153.05, episodes: 802\n",
      "73434: reward: 116.00, mean_100: 152.21, episodes: 803\n",
      "73563: reward: 129.00, mean_100: 151.50, episodes: 804\n",
      "73697: reward: 134.00, mean_100: 150.84, episodes: 805\n",
      "73835: reward: 138.00, mean_100: 150.22, episodes: 806\n",
      "74010: reward: 175.00, mean_100: 149.97, episodes: 807\n",
      "74160: reward: 150.00, mean_100: 149.47, episodes: 808\n",
      "74347: reward: 187.00, mean_100: 149.34, episodes: 809\n",
      "74526: reward: 179.00, mean_100: 149.13, episodes: 810\n",
      "74696: reward: 170.00, mean_100: 148.83, episodes: 811\n",
      "74893: reward: 197.00, mean_100: 148.80, episodes: 812\n",
      "75093: reward: 200.00, mean_100: 148.80, episodes: 813\n",
      "75270: reward: 177.00, mean_100: 148.57, episodes: 814\n",
      "75470: reward: 200.00, mean_100: 148.57, episodes: 815\n",
      "75670: reward: 200.00, mean_100: 148.57, episodes: 816\n",
      "75870: reward: 200.00, mean_100: 148.57, episodes: 817\n",
      "76070: reward: 200.00, mean_100: 148.57, episodes: 818\n",
      "76270: reward: 200.00, mean_100: 148.57, episodes: 819\n",
      "76470: reward: 200.00, mean_100: 148.57, episodes: 820\n",
      "76670: reward: 200.00, mean_100: 148.57, episodes: 821\n",
      "76870: reward: 200.00, mean_100: 148.57, episodes: 822\n",
      "77070: reward: 200.00, mean_100: 148.57, episodes: 823\n",
      "77270: reward: 200.00, mean_100: 148.63, episodes: 824\n",
      "77470: reward: 200.00, mean_100: 148.63, episodes: 825\n",
      "77670: reward: 200.00, mean_100: 148.77, episodes: 826\n",
      "77870: reward: 200.00, mean_100: 148.77, episodes: 827\n",
      "78070: reward: 200.00, mean_100: 148.80, episodes: 828\n",
      "78270: reward: 200.00, mean_100: 148.80, episodes: 829\n",
      "78470: reward: 200.00, mean_100: 148.80, episodes: 830\n",
      "78670: reward: 200.00, mean_100: 148.80, episodes: 831\n",
      "78870: reward: 200.00, mean_100: 148.80, episodes: 832\n",
      "79070: reward: 200.00, mean_100: 148.80, episodes: 833\n",
      "79270: reward: 200.00, mean_100: 148.98, episodes: 834\n",
      "79470: reward: 200.00, mean_100: 149.13, episodes: 835\n",
      "79670: reward: 200.00, mean_100: 149.24, episodes: 836\n",
      "79870: reward: 200.00, mean_100: 149.52, episodes: 837\n",
      "80070: reward: 200.00, mean_100: 150.03, episodes: 838\n",
      "80270: reward: 200.00, mean_100: 150.39, episodes: 839\n",
      "80470: reward: 200.00, mean_100: 150.66, episodes: 840\n",
      "80670: reward: 200.00, mean_100: 150.81, episodes: 841\n",
      "80870: reward: 200.00, mean_100: 150.86, episodes: 842\n",
      "81051: reward: 181.00, mean_100: 150.95, episodes: 843\n",
      "81248: reward: 197.00, mean_100: 151.16, episodes: 844\n",
      "81425: reward: 177.00, mean_100: 151.28, episodes: 845\n",
      "81597: reward: 172.00, mean_100: 151.40, episodes: 846\n",
      "81791: reward: 194.00, mean_100: 151.74, episodes: 847\n",
      "81977: reward: 186.00, mean_100: 152.06, episodes: 848\n",
      "82149: reward: 172.00, mean_100: 152.27, episodes: 849\n",
      "82349: reward: 200.00, mean_100: 152.90, episodes: 850\n",
      "82541: reward: 192.00, mean_100: 153.63, episodes: 851\n",
      "82741: reward: 200.00, mean_100: 154.30, episodes: 852\n",
      "82930: reward: 189.00, mean_100: 155.06, episodes: 853\n",
      "83113: reward: 183.00, mean_100: 155.64, episodes: 854\n",
      "83313: reward: 200.00, mean_100: 156.36, episodes: 855\n",
      "83513: reward: 200.00, mean_100: 156.98, episodes: 856\n",
      "83685: reward: 172.00, mean_100: 157.26, episodes: 857\n",
      "83885: reward: 200.00, mean_100: 158.01, episodes: 858\n",
      "84066: reward: 181.00, mean_100: 158.61, episodes: 859\n",
      "84244: reward: 178.00, mean_100: 159.01, episodes: 860\n",
      "84444: reward: 200.00, mean_100: 159.62, episodes: 861\n",
      "84644: reward: 200.00, mean_100: 160.52, episodes: 862\n",
      "84844: reward: 200.00, mean_100: 161.31, episodes: 863\n",
      "85044: reward: 200.00, mean_100: 162.07, episodes: 864\n",
      "85244: reward: 200.00, mean_100: 163.03, episodes: 865\n",
      "85444: reward: 200.00, mean_100: 163.93, episodes: 866\n",
      "85644: reward: 200.00, mean_100: 165.49, episodes: 867\n",
      "85844: reward: 200.00, mean_100: 166.54, episodes: 868\n",
      "86041: reward: 197.00, mean_100: 167.36, episodes: 869\n",
      "86241: reward: 200.00, mean_100: 168.31, episodes: 870\n",
      "86441: reward: 200.00, mean_100: 169.27, episodes: 871\n",
      "86640: reward: 199.00, mean_100: 170.17, episodes: 872\n",
      "86838: reward: 198.00, mean_100: 171.02, episodes: 873\n",
      "87035: reward: 197.00, mean_100: 171.88, episodes: 874\n",
      "87221: reward: 186.00, mean_100: 172.46, episodes: 875\n",
      "87408: reward: 187.00, mean_100: 173.20, episodes: 876\n",
      "87579: reward: 171.00, mean_100: 173.72, episodes: 877\n",
      "87720: reward: 141.00, mean_100: 173.86, episodes: 878\n",
      "87861: reward: 141.00, mean_100: 174.12, episodes: 879\n",
      "87977: reward: 116.00, mean_100: 174.10, episodes: 880\n",
      "88112: reward: 135.00, mean_100: 174.25, episodes: 881\n",
      "88217: reward: 105.00, mean_100: 174.17, episodes: 882\n",
      "88250: reward:  33.00, mean_100: 173.32, episodes: 883\n",
      "88375: reward: 125.00, mean_100: 173.42, episodes: 884\n",
      "88509: reward: 134.00, mean_100: 173.42, episodes: 885\n",
      "88629: reward: 120.00, mean_100: 173.42, episodes: 886\n",
      "88743: reward: 114.00, mean_100: 173.21, episodes: 887\n",
      "88872: reward: 129.00, mean_100: 173.11, episodes: 888\n",
      "88995: reward: 123.00, mean_100: 173.23, episodes: 889\n",
      "89115: reward: 120.00, mean_100: 173.23, episodes: 890\n",
      "89243: reward: 128.00, mean_100: 173.36, episodes: 891\n",
      "89374: reward: 131.00, mean_100: 173.38, episodes: 892\n",
      "89502: reward: 128.00, mean_100: 173.31, episodes: 893\n",
      "89606: reward: 104.00, mean_100: 172.94, episodes: 894\n",
      "89723: reward: 117.00, mean_100: 172.85, episodes: 895\n",
      "89837: reward: 114.00, mean_100: 172.69, episodes: 896\n",
      "89881: reward:  44.00, mean_100: 171.94, episodes: 897\n",
      "89996: reward: 115.00, mean_100: 171.87, episodes: 898\n",
      "90112: reward: 116.00, mean_100: 171.79, episodes: 899\n",
      "90216: reward: 104.00, mean_100: 171.72, episodes: 900\n",
      "90324: reward: 108.00, mean_100: 171.46, episodes: 901\n",
      "90369: reward:  45.00, mean_100: 170.51, episodes: 902\n",
      "90474: reward: 105.00, mean_100: 170.40, episodes: 903\n",
      "90553: reward:  79.00, mean_100: 169.90, episodes: 904\n",
      "90667: reward: 114.00, mean_100: 169.70, episodes: 905\n",
      "90716: reward:  49.00, mean_100: 168.81, episodes: 906\n",
      "90761: reward:  45.00, mean_100: 167.51, episodes: 907\n",
      "90813: reward:  52.00, mean_100: 166.53, episodes: 908\n",
      "90908: reward:  95.00, mean_100: 165.61, episodes: 909\n",
      "90960: reward:  52.00, mean_100: 164.34, episodes: 910\n",
      "91032: reward:  72.00, mean_100: 163.36, episodes: 911\n",
      "91118: reward:  86.00, mean_100: 162.25, episodes: 912\n",
      "91185: reward:  67.00, mean_100: 160.92, episodes: 913\n",
      "91237: reward:  52.00, mean_100: 159.67, episodes: 914\n",
      "91350: reward: 113.00, mean_100: 158.80, episodes: 915\n",
      "91457: reward: 107.00, mean_100: 157.87, episodes: 916\n",
      "91554: reward:  97.00, mean_100: 156.84, episodes: 917\n",
      "91652: reward:  98.00, mean_100: 155.82, episodes: 918\n",
      "91740: reward:  88.00, mean_100: 154.70, episodes: 919\n",
      "91806: reward:  66.00, mean_100: 153.36, episodes: 920\n",
      "91848: reward:  42.00, mean_100: 151.78, episodes: 921\n",
      "91899: reward:  51.00, mean_100: 150.29, episodes: 922\n",
      "91956: reward:  57.00, mean_100: 148.86, episodes: 923\n",
      "92012: reward:  56.00, mean_100: 147.42, episodes: 924\n",
      "92106: reward:  94.00, mean_100: 146.36, episodes: 925\n",
      "92168: reward:  62.00, mean_100: 144.98, episodes: 926\n",
      "92235: reward:  67.00, mean_100: 143.65, episodes: 927\n",
      "92304: reward:  69.00, mean_100: 142.34, episodes: 928\n",
      "92354: reward:  50.00, mean_100: 140.84, episodes: 929\n",
      "92416: reward:  62.00, mean_100: 139.46, episodes: 930\n",
      "92500: reward:  84.00, mean_100: 138.30, episodes: 931\n",
      "92537: reward:  37.00, mean_100: 136.67, episodes: 932\n",
      "92591: reward:  54.00, mean_100: 135.21, episodes: 933\n",
      "92676: reward:  85.00, mean_100: 134.06, episodes: 934\n",
      "92746: reward:  70.00, mean_100: 132.76, episodes: 935\n",
      "92824: reward:  78.00, mean_100: 131.54, episodes: 936\n",
      "92895: reward:  71.00, mean_100: 130.25, episodes: 937\n",
      "92974: reward:  79.00, mean_100: 129.04, episodes: 938\n",
      "93023: reward:  49.00, mean_100: 127.53, episodes: 939\n",
      "93096: reward:  73.00, mean_100: 126.26, episodes: 940\n",
      "93167: reward:  71.00, mean_100: 124.97, episodes: 941\n",
      "93244: reward:  77.00, mean_100: 123.74, episodes: 942\n",
      "93325: reward:  81.00, mean_100: 122.74, episodes: 943\n",
      "93394: reward:  69.00, mean_100: 121.46, episodes: 944\n",
      "93466: reward:  72.00, mean_100: 120.41, episodes: 945\n",
      "93533: reward:  67.00, mean_100: 119.36, episodes: 946\n",
      "93623: reward:  90.00, mean_100: 118.32, episodes: 947\n",
      "93688: reward:  65.00, mean_100: 117.11, episodes: 948\n",
      "93753: reward:  65.00, mean_100: 116.04, episodes: 949\n",
      "93825: reward:  72.00, mean_100: 114.76, episodes: 950\n",
      "93886: reward:  61.00, mean_100: 113.45, episodes: 951\n",
      "93926: reward:  40.00, mean_100: 111.85, episodes: 952\n",
      "93975: reward:  49.00, mean_100: 110.45, episodes: 953\n",
      "94044: reward:  69.00, mean_100: 109.31, episodes: 954\n",
      "94090: reward:  46.00, mean_100: 107.77, episodes: 955\n",
      "94150: reward:  60.00, mean_100: 106.37, episodes: 956\n",
      "94199: reward:  49.00, mean_100: 105.14, episodes: 957\n",
      "94237: reward:  38.00, mean_100: 103.52, episodes: 958\n",
      "94279: reward:  42.00, mean_100: 102.13, episodes: 959\n",
      "94317: reward:  38.00, mean_100: 100.73, episodes: 960\n",
      "94358: reward:  41.00, mean_100:  99.14, episodes: 961\n",
      "94405: reward:  47.00, mean_100:  97.61, episodes: 962\n",
      "94457: reward:  52.00, mean_100:  96.13, episodes: 963\n",
      "94507: reward:  50.00, mean_100:  94.63, episodes: 964\n",
      "94532: reward:  25.00, mean_100:  92.88, episodes: 965\n",
      "94579: reward:  47.00, mean_100:  91.35, episodes: 966\n",
      "94635: reward:  56.00, mean_100:  89.91, episodes: 967\n",
      "94681: reward:  46.00, mean_100:  88.37, episodes: 968\n",
      "94723: reward:  42.00, mean_100:  86.82, episodes: 969\n",
      "94789: reward:  66.00, mean_100:  85.48, episodes: 970\n",
      "94842: reward:  53.00, mean_100:  84.01, episodes: 971\n",
      "94913: reward:  71.00, mean_100:  82.73, episodes: 972\n",
      "94958: reward:  45.00, mean_100:  81.20, episodes: 973\n",
      "95007: reward:  49.00, mean_100:  79.72, episodes: 974\n",
      "95053: reward:  46.00, mean_100:  78.32, episodes: 975\n",
      "95126: reward:  73.00, mean_100:  77.18, episodes: 976\n",
      "95222: reward:  96.00, mean_100:  76.43, episodes: 977\n",
      "95261: reward:  39.00, mean_100:  75.41, episodes: 978\n",
      "95357: reward:  96.00, mean_100:  74.96, episodes: 979\n",
      "95478: reward: 121.00, mean_100:  75.01, episodes: 980\n",
      "95559: reward:  81.00, mean_100:  74.47, episodes: 981\n",
      "95655: reward:  96.00, mean_100:  74.38, episodes: 982\n",
      "95769: reward: 114.00, mean_100:  75.19, episodes: 983\n",
      "95855: reward:  86.00, mean_100:  74.80, episodes: 984\n",
      "95942: reward:  87.00, mean_100:  74.33, episodes: 985\n",
      "96038: reward:  96.00, mean_100:  74.09, episodes: 986\n",
      "96149: reward: 111.00, mean_100:  74.06, episodes: 987\n",
      "96266: reward: 117.00, mean_100:  73.94, episodes: 988\n",
      "96372: reward: 106.00, mean_100:  73.77, episodes: 989\n",
      "96481: reward: 109.00, mean_100:  73.66, episodes: 990\n",
      "96596: reward: 115.00, mean_100:  73.53, episodes: 991\n",
      "96705: reward: 109.00, mean_100:  73.31, episodes: 992\n",
      "96835: reward: 130.00, mean_100:  73.33, episodes: 993\n",
      "96958: reward: 123.00, mean_100:  73.52, episodes: 994\n",
      "97088: reward: 130.00, mean_100:  73.65, episodes: 995\n",
      "97208: reward: 120.00, mean_100:  73.71, episodes: 996\n",
      "97346: reward: 138.00, mean_100:  74.65, episodes: 997\n",
      "97473: reward: 127.00, mean_100:  74.77, episodes: 998\n",
      "97622: reward: 149.00, mean_100:  75.10, episodes: 999\n",
      "97758: reward: 136.00, mean_100:  75.42, episodes: 1000\n",
      "97894: reward: 136.00, mean_100:  75.70, episodes: 1001\n",
      "98010: reward: 116.00, mean_100:  76.41, episodes: 1002\n",
      "98136: reward: 126.00, mean_100:  76.62, episodes: 1003\n",
      "98292: reward: 156.00, mean_100:  77.39, episodes: 1004\n",
      "98457: reward: 165.00, mean_100:  77.90, episodes: 1005\n",
      "98650: reward: 193.00, mean_100:  79.34, episodes: 1006\n",
      "98850: reward: 200.00, mean_100:  80.89, episodes: 1007\n",
      "99032: reward: 182.00, mean_100:  82.19, episodes: 1008\n",
      "99197: reward: 165.00, mean_100:  82.89, episodes: 1009\n",
      "99357: reward: 160.00, mean_100:  83.97, episodes: 1010\n",
      "99511: reward: 154.00, mean_100:  84.79, episodes: 1011\n",
      "99697: reward: 186.00, mean_100:  85.79, episodes: 1012\n",
      "99847: reward: 150.00, mean_100:  86.62, episodes: 1013\n",
      "100005: reward: 158.00, mean_100:  87.68, episodes: 1014\n",
      "100161: reward: 156.00, mean_100:  88.11, episodes: 1015\n",
      "100330: reward: 169.00, mean_100:  88.73, episodes: 1016\n",
      "100490: reward: 160.00, mean_100:  89.36, episodes: 1017\n",
      "100643: reward: 153.00, mean_100:  89.91, episodes: 1018\n",
      "100811: reward: 168.00, mean_100:  90.71, episodes: 1019\n",
      "100989: reward: 178.00, mean_100:  91.83, episodes: 1020\n",
      "101155: reward: 166.00, mean_100:  93.07, episodes: 1021\n",
      "101308: reward: 153.00, mean_100:  94.09, episodes: 1022\n",
      "101461: reward: 153.00, mean_100:  95.05, episodes: 1023\n",
      "101616: reward: 155.00, mean_100:  96.04, episodes: 1024\n",
      "101781: reward: 165.00, mean_100:  96.75, episodes: 1025\n",
      "101968: reward: 187.00, mean_100:  98.00, episodes: 1026\n",
      "102139: reward: 171.00, mean_100:  99.04, episodes: 1027\n",
      "102284: reward: 145.00, mean_100:  99.80, episodes: 1028\n",
      "102445: reward: 161.00, mean_100: 100.91, episodes: 1029\n",
      "102603: reward: 158.00, mean_100: 101.87, episodes: 1030\n",
      "102779: reward: 176.00, mean_100: 102.79, episodes: 1031\n",
      "102967: reward: 188.00, mean_100: 104.30, episodes: 1032\n",
      "103133: reward: 166.00, mean_100: 105.42, episodes: 1033\n",
      "103279: reward: 146.00, mean_100: 106.03, episodes: 1034\n",
      "103425: reward: 146.00, mean_100: 106.79, episodes: 1035\n",
      "103546: reward: 121.00, mean_100: 107.22, episodes: 1036\n",
      "103691: reward: 145.00, mean_100: 107.96, episodes: 1037\n",
      "103847: reward: 156.00, mean_100: 108.73, episodes: 1038\n",
      "103989: reward: 142.00, mean_100: 109.66, episodes: 1039\n",
      "104152: reward: 163.00, mean_100: 110.56, episodes: 1040\n",
      "104319: reward: 167.00, mean_100: 111.52, episodes: 1041\n",
      "104509: reward: 190.00, mean_100: 112.65, episodes: 1042\n",
      "104684: reward: 175.00, mean_100: 113.59, episodes: 1043\n",
      "104851: reward: 167.00, mean_100: 114.57, episodes: 1044\n",
      "105051: reward: 200.00, mean_100: 115.85, episodes: 1045\n",
      "105230: reward: 179.00, mean_100: 116.97, episodes: 1046\n",
      "105411: reward: 181.00, mean_100: 117.88, episodes: 1047\n",
      "105611: reward: 200.00, mean_100: 119.23, episodes: 1048\n",
      "105811: reward: 200.00, mean_100: 120.58, episodes: 1049\n",
      "106011: reward: 200.00, mean_100: 121.86, episodes: 1050\n",
      "106211: reward: 200.00, mean_100: 123.25, episodes: 1051\n",
      "106411: reward: 200.00, mean_100: 124.85, episodes: 1052\n",
      "106611: reward: 200.00, mean_100: 126.36, episodes: 1053\n",
      "106811: reward: 200.00, mean_100: 127.67, episodes: 1054\n",
      "107011: reward: 200.00, mean_100: 129.21, episodes: 1055\n",
      "107211: reward: 200.00, mean_100: 130.61, episodes: 1056\n",
      "107411: reward: 200.00, mean_100: 132.12, episodes: 1057\n",
      "107611: reward: 200.00, mean_100: 133.74, episodes: 1058\n",
      "107811: reward: 200.00, mean_100: 135.32, episodes: 1059\n",
      "108011: reward: 200.00, mean_100: 136.94, episodes: 1060\n",
      "108211: reward: 200.00, mean_100: 138.53, episodes: 1061\n",
      "108411: reward: 200.00, mean_100: 140.06, episodes: 1062\n",
      "108611: reward: 200.00, mean_100: 141.54, episodes: 1063\n",
      "108811: reward: 200.00, mean_100: 143.04, episodes: 1064\n",
      "109011: reward: 200.00, mean_100: 144.79, episodes: 1065\n",
      "109211: reward: 200.00, mean_100: 146.32, episodes: 1066\n",
      "109411: reward: 200.00, mean_100: 147.76, episodes: 1067\n",
      "109611: reward: 200.00, mean_100: 149.30, episodes: 1068\n",
      "109811: reward: 200.00, mean_100: 150.88, episodes: 1069\n",
      "110011: reward: 200.00, mean_100: 152.22, episodes: 1070\n",
      "110211: reward: 200.00, mean_100: 153.69, episodes: 1071\n",
      "110411: reward: 200.00, mean_100: 154.98, episodes: 1072\n",
      "110611: reward: 200.00, mean_100: 156.53, episodes: 1073\n",
      "110811: reward: 200.00, mean_100: 158.04, episodes: 1074\n",
      "111011: reward: 200.00, mean_100: 159.58, episodes: 1075\n",
      "111211: reward: 200.00, mean_100: 160.85, episodes: 1076\n",
      "111411: reward: 200.00, mean_100: 161.89, episodes: 1077\n",
      "111611: reward: 200.00, mean_100: 163.50, episodes: 1078\n",
      "111811: reward: 200.00, mean_100: 164.54, episodes: 1079\n",
      "112011: reward: 200.00, mean_100: 165.33, episodes: 1080\n",
      "112211: reward: 200.00, mean_100: 166.52, episodes: 1081\n",
      "112411: reward: 200.00, mean_100: 167.56, episodes: 1082\n",
      "112611: reward: 200.00, mean_100: 168.42, episodes: 1083\n",
      "112811: reward: 200.00, mean_100: 169.56, episodes: 1084\n",
      "113011: reward: 200.00, mean_100: 170.69, episodes: 1085\n",
      "113211: reward: 200.00, mean_100: 171.73, episodes: 1086\n",
      "113411: reward: 200.00, mean_100: 172.62, episodes: 1087\n",
      "113611: reward: 200.00, mean_100: 173.45, episodes: 1088\n",
      "113811: reward: 200.00, mean_100: 174.39, episodes: 1089\n",
      "114011: reward: 200.00, mean_100: 175.30, episodes: 1090\n",
      "114211: reward: 200.00, mean_100: 176.15, episodes: 1091\n",
      "114411: reward: 200.00, mean_100: 177.06, episodes: 1092\n",
      "114611: reward: 200.00, mean_100: 177.76, episodes: 1093\n",
      "114811: reward: 200.00, mean_100: 178.53, episodes: 1094\n",
      "115011: reward: 200.00, mean_100: 179.23, episodes: 1095\n",
      "115211: reward: 200.00, mean_100: 180.03, episodes: 1096\n",
      "115411: reward: 200.00, mean_100: 180.65, episodes: 1097\n",
      "115611: reward: 200.00, mean_100: 181.38, episodes: 1098\n",
      "115811: reward: 200.00, mean_100: 181.89, episodes: 1099\n",
      "116011: reward: 200.00, mean_100: 182.53, episodes: 1100\n",
      "116211: reward: 200.00, mean_100: 183.17, episodes: 1101\n",
      "116411: reward: 200.00, mean_100: 184.01, episodes: 1102\n",
      "116611: reward: 200.00, mean_100: 184.75, episodes: 1103\n",
      "116811: reward: 200.00, mean_100: 185.19, episodes: 1104\n",
      "117011: reward: 200.00, mean_100: 185.54, episodes: 1105\n",
      "117211: reward: 200.00, mean_100: 185.61, episodes: 1106\n",
      "117411: reward: 200.00, mean_100: 185.61, episodes: 1107\n",
      "117611: reward: 200.00, mean_100: 185.79, episodes: 1108\n",
      "117784: reward: 173.00, mean_100: 185.87, episodes: 1109\n",
      "117984: reward: 200.00, mean_100: 186.27, episodes: 1110\n",
      "118184: reward: 200.00, mean_100: 186.73, episodes: 1111\n",
      "118384: reward: 200.00, mean_100: 186.87, episodes: 1112\n",
      "118584: reward: 200.00, mean_100: 187.37, episodes: 1113\n",
      "118762: reward: 178.00, mean_100: 187.57, episodes: 1114\n",
      "118933: reward: 171.00, mean_100: 187.72, episodes: 1115\n",
      "119095: reward: 162.00, mean_100: 187.65, episodes: 1116\n",
      "119249: reward: 154.00, mean_100: 187.59, episodes: 1117\n",
      "119397: reward: 148.00, mean_100: 187.54, episodes: 1118\n",
      "119521: reward: 124.00, mean_100: 187.10, episodes: 1119\n",
      "119656: reward: 135.00, mean_100: 186.67, episodes: 1120\n",
      "119805: reward: 149.00, mean_100: 186.50, episodes: 1121\n",
      "119942: reward: 137.00, mean_100: 186.34, episodes: 1122\n",
      "120079: reward: 137.00, mean_100: 186.18, episodes: 1123\n",
      "120218: reward: 139.00, mean_100: 186.02, episodes: 1124\n",
      "120354: reward: 136.00, mean_100: 185.73, episodes: 1125\n",
      "120522: reward: 168.00, mean_100: 185.54, episodes: 1126\n",
      "120683: reward: 161.00, mean_100: 185.44, episodes: 1127\n",
      "120833: reward: 150.00, mean_100: 185.49, episodes: 1128\n",
      "120999: reward: 166.00, mean_100: 185.54, episodes: 1129\n",
      "121165: reward: 166.00, mean_100: 185.62, episodes: 1130\n",
      "121312: reward: 147.00, mean_100: 185.33, episodes: 1131\n",
      "121480: reward: 168.00, mean_100: 185.13, episodes: 1132\n",
      "121632: reward: 152.00, mean_100: 184.99, episodes: 1133\n",
      "121791: reward: 159.00, mean_100: 185.12, episodes: 1134\n",
      "121975: reward: 184.00, mean_100: 185.50, episodes: 1135\n",
      "122175: reward: 200.00, mean_100: 186.29, episodes: 1136\n",
      "122344: reward: 169.00, mean_100: 186.53, episodes: 1137\n",
      "122544: reward: 200.00, mean_100: 186.97, episodes: 1138\n",
      "122719: reward: 175.00, mean_100: 187.30, episodes: 1139\n",
      "122919: reward: 200.00, mean_100: 187.67, episodes: 1140\n",
      "123119: reward: 200.00, mean_100: 188.00, episodes: 1141\n",
      "123319: reward: 200.00, mean_100: 188.10, episodes: 1142\n",
      "123519: reward: 200.00, mean_100: 188.35, episodes: 1143\n",
      "123719: reward: 200.00, mean_100: 188.68, episodes: 1144\n",
      "123919: reward: 200.00, mean_100: 188.68, episodes: 1145\n",
      "124119: reward: 200.00, mean_100: 188.89, episodes: 1146\n",
      "124319: reward: 200.00, mean_100: 189.08, episodes: 1147\n",
      "124519: reward: 200.00, mean_100: 189.08, episodes: 1148\n",
      "124719: reward: 200.00, mean_100: 189.08, episodes: 1149\n",
      "124919: reward: 200.00, mean_100: 189.08, episodes: 1150\n",
      "125119: reward: 200.00, mean_100: 189.08, episodes: 1151\n",
      "125319: reward: 200.00, mean_100: 189.08, episodes: 1152\n",
      "125519: reward: 200.00, mean_100: 189.08, episodes: 1153\n",
      "125719: reward: 200.00, mean_100: 189.08, episodes: 1154\n",
      "125919: reward: 200.00, mean_100: 189.08, episodes: 1155\n",
      "126119: reward: 200.00, mean_100: 189.08, episodes: 1156\n",
      "126319: reward: 200.00, mean_100: 189.08, episodes: 1157\n",
      "126519: reward: 200.00, mean_100: 189.08, episodes: 1158\n",
      "126719: reward: 200.00, mean_100: 189.08, episodes: 1159\n",
      "126919: reward: 200.00, mean_100: 189.08, episodes: 1160\n",
      "127119: reward: 200.00, mean_100: 189.08, episodes: 1161\n",
      "127319: reward: 200.00, mean_100: 189.08, episodes: 1162\n",
      "127519: reward: 200.00, mean_100: 189.08, episodes: 1163\n",
      "127719: reward: 200.00, mean_100: 189.08, episodes: 1164\n",
      "127919: reward: 200.00, mean_100: 189.08, episodes: 1165\n",
      "128119: reward: 200.00, mean_100: 189.08, episodes: 1166\n",
      "128319: reward: 200.00, mean_100: 189.08, episodes: 1167\n",
      "128519: reward: 200.00, mean_100: 189.08, episodes: 1168\n",
      "128719: reward: 200.00, mean_100: 189.08, episodes: 1169\n",
      "128919: reward: 200.00, mean_100: 189.08, episodes: 1170\n",
      "129119: reward: 200.00, mean_100: 189.08, episodes: 1171\n",
      "129319: reward: 200.00, mean_100: 189.08, episodes: 1172\n",
      "129519: reward: 200.00, mean_100: 189.08, episodes: 1173\n",
      "129719: reward: 200.00, mean_100: 189.08, episodes: 1174\n",
      "129919: reward: 200.00, mean_100: 189.08, episodes: 1175\n",
      "130119: reward: 200.00, mean_100: 189.08, episodes: 1176\n",
      "130319: reward: 200.00, mean_100: 189.08, episodes: 1177\n",
      "130519: reward: 200.00, mean_100: 189.08, episodes: 1178\n",
      "130719: reward: 200.00, mean_100: 189.08, episodes: 1179\n",
      "130919: reward: 200.00, mean_100: 189.08, episodes: 1180\n",
      "131119: reward: 200.00, mean_100: 189.08, episodes: 1181\n",
      "131319: reward: 200.00, mean_100: 189.08, episodes: 1182\n",
      "131519: reward: 200.00, mean_100: 189.08, episodes: 1183\n",
      "131719: reward: 200.00, mean_100: 189.08, episodes: 1184\n",
      "131919: reward: 200.00, mean_100: 189.08, episodes: 1185\n",
      "132119: reward: 200.00, mean_100: 189.08, episodes: 1186\n",
      "132319: reward: 200.00, mean_100: 189.08, episodes: 1187\n",
      "132519: reward: 200.00, mean_100: 189.08, episodes: 1188\n",
      "132719: reward: 200.00, mean_100: 189.08, episodes: 1189\n",
      "132919: reward: 200.00, mean_100: 189.08, episodes: 1190\n",
      "133119: reward: 200.00, mean_100: 189.08, episodes: 1191\n",
      "133319: reward: 200.00, mean_100: 189.08, episodes: 1192\n",
      "133519: reward: 200.00, mean_100: 189.08, episodes: 1193\n",
      "133719: reward: 200.00, mean_100: 189.08, episodes: 1194\n",
      "133919: reward: 200.00, mean_100: 189.08, episodes: 1195\n",
      "134119: reward: 200.00, mean_100: 189.08, episodes: 1196\n",
      "134319: reward: 200.00, mean_100: 189.08, episodes: 1197\n",
      "134519: reward: 200.00, mean_100: 189.08, episodes: 1198\n",
      "134719: reward: 200.00, mean_100: 189.08, episodes: 1199\n",
      "134919: reward: 200.00, mean_100: 189.08, episodes: 1200\n",
      "135119: reward: 200.00, mean_100: 189.08, episodes: 1201\n",
      "135319: reward: 200.00, mean_100: 189.08, episodes: 1202\n",
      "135519: reward: 200.00, mean_100: 189.08, episodes: 1203\n",
      "135719: reward: 200.00, mean_100: 189.08, episodes: 1204\n",
      "135919: reward: 200.00, mean_100: 189.08, episodes: 1205\n",
      "136119: reward: 200.00, mean_100: 189.08, episodes: 1206\n",
      "136319: reward: 200.00, mean_100: 189.08, episodes: 1207\n",
      "136519: reward: 200.00, mean_100: 189.08, episodes: 1208\n",
      "136719: reward: 200.00, mean_100: 189.35, episodes: 1209\n",
      "136919: reward: 200.00, mean_100: 189.35, episodes: 1210\n",
      "137119: reward: 200.00, mean_100: 189.35, episodes: 1211\n",
      "137319: reward: 200.00, mean_100: 189.35, episodes: 1212\n",
      "137519: reward: 200.00, mean_100: 189.35, episodes: 1213\n",
      "137719: reward: 200.00, mean_100: 189.57, episodes: 1214\n",
      "137919: reward: 200.00, mean_100: 189.86, episodes: 1215\n",
      "138119: reward: 200.00, mean_100: 190.24, episodes: 1216\n",
      "138319: reward: 200.00, mean_100: 190.70, episodes: 1217\n",
      "138519: reward: 200.00, mean_100: 191.22, episodes: 1218\n",
      "138719: reward: 200.00, mean_100: 191.98, episodes: 1219\n",
      "138919: reward: 200.00, mean_100: 192.63, episodes: 1220\n",
      "139119: reward: 200.00, mean_100: 193.14, episodes: 1221\n",
      "139319: reward: 200.00, mean_100: 193.77, episodes: 1222\n",
      "139519: reward: 200.00, mean_100: 194.40, episodes: 1223\n",
      "139719: reward: 200.00, mean_100: 195.01, episodes: 1224\n",
      "Solved in 139719 steps and 1224 episodes!\n"
     ]
    }
   ],
   "source": [
    "# Beginn der Trainingsschleife\n",
    "for step_idx, exp in enumerate(exp_source):\n",
    "    reward_sum += exp.reward\n",
    "    baseline = reward_sum / (step_idx + 1)\n",
    "    writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "    batch_states.append(exp.state)\n",
    "    batch_actions.append(int(exp.action))\n",
    "    if args.baseline:\n",
    "        batch_scales.append(exp.reward - baseline)\n",
    "    else:\n",
    "        batch_scales.append(exp.reward)\n",
    "        \n",
    "        \n",
    "    # handle new rewards\n",
    "    new_rewards = exp_source.pop_total_rewards() # total_rewards ist eine Liste von Rewards\n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_rewards = float(np.mean(total_rewards[-100:])) # Durchschnitt der letzten 100 Rewards\n",
    "        print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "            step_idx, reward, mean_rewards, done_episodes))\n",
    "        writer.add_scalar(\"reward\", reward, step_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "        writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "        if mean_rewards > 195:\n",
    "            print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "            break\n",
    "\n",
    "    if len(batch_states) < BATCH_SIZE:\n",
    "            continue\n",
    "        \n",
    "    states_v = torch.FloatTensor(batch_states)\n",
    "    batch_actions_t = torch.LongTensor(batch_actions)\n",
    "    batch_scale_v = torch.FloatTensor(batch_scales)\n",
    "\n",
    "    optimizer.zero_grad() # Gradienten zu 0 setzen\n",
    "    logits_v = net(states_v) # Berechnung der Rohausgaben des Models\n",
    "    log_prob_v = F.log_softmax(logits_v, dim=1) # logarithmierte Wahrscheinlichkeiten berechnen\n",
    "    \n",
    "    # Berechnung der Policy Loss\n",
    "    log_p_a_v = log_prob_v[range(BATCH_SIZE), batch_actions_t] \n",
    "    log_prob_actions_v = batch_scale_v * log_p_a_v\n",
    "    loss_policy_v = -log_prob_actions_v.mean()\n",
    "    \n",
    "    # Rückwertsdurchgang\n",
    "    loss_policy_v.backward(retain_graph=True)\n",
    "    grads = np.concatenate([p.grad.data.numpy().flatten()\n",
    "                            for p in net.parameters()\n",
    "                            if p.grad is not None])\n",
    "    \n",
    "    # Berechnung der Entropy Loss\n",
    "    prob_v = F.softmax(logits_v, dim=1)\n",
    "    entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "    entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "    entropy_loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Berechnung des Gesamt Loss\n",
    "    loss_v = loss_policy_v + entropy_loss_v\n",
    "\n",
    "    # calc KL-div\n",
    "    new_logits_v = net(states_v)\n",
    "    new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "    kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()\n",
    "    writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "\n",
    "    writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "    writer.add_scalar(\"entropy\", entropy_v.item(), step_idx)\n",
    "    writer.add_scalar(\"batch_scales\", np.mean(batch_scales), step_idx)\n",
    "    writer.add_scalar(\"loss_entropy\", entropy_loss_v.item(), step_idx)\n",
    "    writer.add_scalar(\"loss_policy\", loss_policy_v.item(), step_idx)\n",
    "    writer.add_scalar(\"loss_total\", loss_v.item(), step_idx)\n",
    "\n",
    "    g_l2 = np.sqrt(np.mean(np.square(grads)))\n",
    "    g_max = np.max(np.abs(grads))\n",
    "    writer.add_scalar(\"grad_l2\", g_l2, step_idx)\n",
    "    writer.add_scalar(\"grad_max\", g_max, step_idx)\n",
    "    writer.add_scalar(\"grad_var\", np.var(grads), step_idx)\n",
    "\n",
    "    batch_states.clear()\n",
    "    batch_actions.clear()\n",
    "    batch_scales.clear()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Policy-Loss*: Der Policy-Loss dient dazu, das Netzwerk zu trainieren, eine bessere Politik (Strategie) zu lernen, die zu höheren Belohnungen führt. Durch die Maximierung des Policy-Losses werden gute Aktionen, die höhere Belohnungen erzielen, bevorzugt. In diesem Sinne optimiert der Policy-Loss die Richtung und Stärke der Aktionen des Agenten, um die Belohnung zu maximieren. Der Policy-Loss fördert somit das Lernen, welche Aktionen gute Ergebnisse erzeugen und verstärkt diese Aktionen weiter.\n",
    "\n",
    "- ### Entropy-Loss\n",
    "Der Entropy-Loss dient dazu, die Exploration des Agenten zu fördern und die Diversität der Aktionen zu erhöhen. Indem der Entropy-Loss in den Gesamt-Loss einbezogen wird, wird der Grad der Unsicherheit in der Aktionserzeugung maximiert. Dies hilft, die Exploration des Agenten zu erhöhen und sicherzustellen, dass das Netzwerk verschiedene Aktionen ausprobiert und nicht in einer vorzeitigen Konvergenz auf eine einzelne Aktion stecken bleibt. Der Entropy-Loss trägt dazu bei, die Wahrscheinlichkeitsverteilung der Aktionen zu regulieren und die Politik zu diversifizieren.\n",
    "\n",
    "Der Policy-Loss und der Entropy-Loss werden oft kombiniert, um eine geeignete Gleichgewicht zwischen der Exploration und der Ausbeutung von guten Aktionen zu finden. Der Entropy-Loss stellt sicher, dass der Agent nicht in einer lokalen Optima stecken bleibt und ermöglicht die Exploration von neuen Aktionen, während der Policy-Loss die Richtung der Aktionen optimiert, um die Belohnungen zu maximieren.\n",
    "\n",
    "\n",
    "- ### Summarywriter\n",
    "Bevor das Training beginnt, erstellt man eine Instanz des SummaryWriter und gibt ihm den Pfad zum Verzeichnis an, in dem die Summaries gespeichert werden sollen. Während des Trainings oder der Evaluation können dann verschiedene Arten von Summaries, wie z.B. Skalare (z.B. Verlust oder Genauigkeit), Histogramme oder Bilder, mit Hilfe des SummaryWriters erstellt und regelmäßig in das angegebene Verzeichnis geschrieben werden.\n",
    "\n",
    "- ### Neue Belohnungen\n",
    "Neue Belohnungen: Wenn neue Belohnungen verfügbar sind, werden sie verarbeitet, indem sie zur Gesamtbelohnungsliste hinzugefügt werden und der Durchschnitt der letzten 100 Belohnungen berechnet wird. Diese Werte werden zur Tensorboard-Zusammenfassung hinzugefügt. Wenn der Durchschnitt der letzten 100 Belohnungen über 195 liegt, wird das Training als erfolgreich abgeschlossen angesehen und die Schleife wird beendet.\n",
    "\n",
    "- ### Batch-Größe\n",
    "Batch-Größe: Wenn genügend Erfahrungen gesammelt wurden, wird der nächste Schritt ausgeführt. Der aktuelle Batch von Zuständen, Aktionen und Skalierungen wird in Tensoren umgerechnet. \n",
    "\n",
    "- ### Vorwärtsdurchgang\n",
    "Der Vorwärtsdurchgang des neuronalen Netzwerks wird durchgeführt und die log_softmax-Werte werden berechnet. Es wird auch der log_likelihood-Wert für die ausgewählten Aktionen berechnet.\n",
    "\n",
    "- ### Richtungsrückgabe\n",
    "Der Rückwärtsdurchgang wird durchgeführt und die Gradienten der Netzwerkparameter werden berechnet. Außerdem werden verschiedene Loss-Werte, wie der Policy-Loss und der Entropy-Loss, berechnet und zur Tensorboard-Zusammenfassung hinzugefügt.\n",
    "\n",
    "- ### KL-Divergenz\n",
    "Die KL-Divergenz zwischen den alten und den neuen Wahrscheinlichkeitsverteilungen wird berechnet und zur Tensorboard-Zusammenfassung hinzugefügt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('DRL_PracticeBook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0255086c2e6ee30c3c5478e95bd942156d3d67958fc93b9f4a13040f0409697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
